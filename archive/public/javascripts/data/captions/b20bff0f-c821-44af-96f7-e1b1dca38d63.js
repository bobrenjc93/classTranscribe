// // Data structure that holds captions for the videos
// var videoCaptions = [
//   [{"text":"Okay, so the rest of what you said is, 'are there any words in the titles that are used a lot in spam?' So basically you wanna look at other spam messages, and [student answers, inaudible] figure out what vocabulary they might use, and hope that there is something in terms of the vocabularies of the titles, as you suggested, that is common.","width":1921},{"text":"Other suggestions?","width":212},{"text":"[student answers inaudibly] What do you mean by training a program?","width":886},{"text":"[student explains, inaudible] So what do you mean by 'similar emails?' So the question is: in what sense are the next messages, which you've never seen, similar to what you have seen before.","width":2138},{"text":"That's key here.","width":59},{"text":"Yeah?","width":158},{"text":"[student answers inaudibly] Okay, so here is a suggestion to formulate the problem: take an email message; map it somehow to a vector, let's say a real valued vector of dimensionality 75.","width":1887},{"text":"If you can do this to all email messages, then there is a well-defined notion of a distance, maybe the cosine between these vectors.","width":653},{"text":"That solves your problem, because now you have a notion of distance, and things that are similar to the spam that you've seen, we're gonna say, 'these are spam'.","width":743},{"text":"Things that are not, you'll say, 'these are NOT spam'.","width":259},{"text":"What is the technical difficulty here?","width":306},{"text":"What is the difficult step in doing what we just said?","width":255},{"text":"[student answers inaudibly] What do you mean by evaluating the similarity?","width":253},{"text":"[student explains, inaudible] Even before evaluating the similarity, the difficulty.","width":723},{"text":"Over there.","width":116},{"text":"[student answers, inaudible] Right, so the key issue is what's in this message is important for it being spam, and what is not.","width":948},{"text":"So just mapping it to a vector, just a technical thing- you can- it's already a vector, the way it's written here.","width":471},{"text":"I don't necessarily need to map it to another vector, maybe I will, but the key issue is what do I look at in this message?","width":492},{"text":"Do I look at the fact that some of this font is blue, and some is red?","width":238},{"text":"Do I look at the size of the font?","width":142},{"text":"Do I look at the characters?","width":195},{"text":"Do I look at...","width":87},{"text":"what do I look at?","width":110},{"text":"Do I look at the sender?","width":93},{"text":"Do I look at the date?","width":62},{"text":"What do I look at in the message to determine that this is spam.","width":437},{"text":"If I don't know what to look at in the message, if I don't tell my program what it really sees here, then we have no chance, right?","width":850},{"text":"Because the key thing is that you have seen this message, for the first time I assume, nevertheless, you can say something about whether for you it's spam.","width":829},{"text":"By the way, it could be that it's spam for you, and not spam for someone else.","width":410},{"text":"So there is a question of developing a model, or a classifier, and this is really what we wanna do in this class.","width":684},{"text":"We wanna be able to study algorithms and techniques that will allow us to lean models from past data.","width":659},{"text":"Here are a few other examples that, again, I wanna kind of trigger your thinking about this because these are...","width":370},{"text":"all of them have to do with stacks, or natural language, but they are very very different types of examples.","width":377},{"text":"So the first one, at the top here, can I have a peace of cake?","width":295},{"text":"We probably didn't mean this kind of peace, this is very similar to the poem problem that I asked before, we probably mean this kind of piece.","width":520},{"text":"Again, you can ask yourself; this is a binary decision.","width":218},{"text":"You see it in a word in a sentence, and you wanna decide: is this the appropriate word to use there?","width":377},{"text":"Is this the appropriate word or should I use something else?","width":269},{"text":"How would you decide?","width":150},{"text":"How would you write a program to decide?","width":354}],
//   [{"text":"Right?","width":124},{"text":"So how you abstract, how you represent things is really crucial here.","width":326},{"text":"Next problem, this is a sentence that says, 'this can will rust', and the task here, is simply to determine what is the part of speech tag of each word in the context of this sentence.","width":1073},{"text":"'This' is an article, 'can' is a noun, 'will' is a kind of a model verb, 'rust' is a verb.","width":502},{"text":"Very easy, sometime you can question, 'why do I need to know this', but it's sometimes important to know whether 'can', in the context of this sentence...","width":757},{"text":"in most cases, 'can' is actually a verb, in this case, 'can' is a noun, and you know this.","width":562},{"text":"Again, it's a prediction problem.","width":242},{"text":"There are multiple part of speech tags, for example, in english, common vocabulary or part of speech tags would have forty five different labels, and you want to determine which one it is.","width":893},{"text":"What makes this problem somewhat different than the earlier problems on the same slide?","width":1020},{"text":"Again, it's a prediction.","width":66},{"text":"I look at the word, 'can', or 'will', that also can have multiple part of speech tags (think about it for a second).","width":529},{"text":"Here, it's a model verb, it could be a noun, it could be other things.","width":332},{"text":"What makes this prediction challenge different than, say, the 'peace/piece' problem?","width":758},{"text":"Yeah?","width":100},{"text":"[student answers, inaudible] So the first one was binary, and this is not binary.","width":637},{"text":"That's one thing.","width":92},{"text":"There are more labels here.","width":160},{"text":"That's one difference.","width":74},{"text":"Another one?","width":57},{"text":"Yeah?","width":55},{"text":"[student answers, inaudible] It depends on the context, but the first sentence also, the context matters.","width":644},{"text":"So, whether it's 'peace', or...","width":557},{"text":"This sentence makes sense either way, and what I wanna know is some kind of property outside the sentence.","width":460},{"text":"That's what I was looking for.","width":458},{"text":"So there are other differences, and you are all right, but this is a key different thing.","width":355},{"text":"Whether one word is a noun depends not only on the sentence it's in, but also on what is the role in the sentence of other things.","width":636},{"text":"For example, in english, it's unlikely that you have two consecutive verbs.","width":294},{"text":"Could happen, unlikely though.","width":128},{"text":"So if I decided that this word is a verb, it's unlikely that I'll decide that the next one is a verb.","width":394},{"text":"So maybe, I don't need to think about this as one prediction per word, but rather I need to think about it globally, the whole sentence together, because the decisions I'm going to make, actually are interdependent.","width":1093},{"text":"So in this case it's a sequence.","width":114},{"text":"In some other cases it will not be a sequence, it will be some kind of other structure, but there will be interdependencies between predictions that I'm making, and that's important.","width":791},{"text":"That's a problem that we sometimes call structured prediction problem.","width":294},{"text":"Finally, the last example that I have here, two sentences, 'the dog bit the kid.","width":444},{"text":"He was taken to a...' vet, or he was taken to a hospital.","width":294},{"text":"Hopefully you agree that what 'he' refers to is determined by which sentence I choose, a vet or a hospital, agreed?","width":1169},{"text":"How do you know?","width":133},{"text":"What do you have to equip your program with, in order for the program to know?","width":621},{"text":"So here is an option: we need to equip the program with some knowledge of what is a vet, what is a hospital, what do the serve.","width":684},{"text":"Yeah?","width":7},{"text":"[student asks question, inaudible] What is a dog?","width":301},{"text":"What is a kid?","width":626},{"text":"[student asks question, inaudible] .", "width":1027}],
//   [{"text":"Say something, what do you see Objects yeah That's correct, that's the easy level of the answer but I think you can say more.","width":1109},{"text":"Animals, cars Yeah the cars and the animals are in different orientations.","width":570},{"text":"That is unintentional...","width":207},{"text":"uhh Back So yeah, four legged animals You probably see few others, few other things that you see here.","width":1773},{"text":"I know but what I am asking is what do you see, what have you recognized?","width":379},{"text":"Vegetables, Ok so basically there are a lot of objects and what you have done is that you have clustered them into groups.","width":804},{"text":"You said I see shoes, cars, vegetables.","width":259},{"text":"Now notice that we made it difficult for you but it wasn't difficult.","width":389},{"text":"The sizes are completely wrong here, right The relative size of these things on the image vs the relative sizes in life.","width":882},{"text":"If you really look at it into the pixel level.","width":221},{"text":"Just choose a shoe and a car for example.","width":259},{"text":"Some of the shoes are a lot more similar at the pixel level to some of the cars than to other shoes and vice versa.","width":776},{"text":"Nevertheless it doesn't bother you at all, you were able to cluster it and moreover much how the problem you were able to name the clusters, which by the way is still a problem today.","width":888},{"text":"We can do a lot about clustering, naming clusters is actually a very difficult problem for different reasons.","width":532},{"text":"So this is another aspect of machine learning, we are gonna discuss this semester.","width":337},{"text":"If you think about this specific example you will see that it is extremely difficult because we have taken a lot of the natural features of these objects away for example size, orientation, colours context etc.","width":1057},{"text":"but it doesn't bother you at all.","width":214},{"text":"Ok, so what is machine learning?","width":319},{"text":"Where do we need it?","width":94},{"text":"Hopefully, the example I have given so far indicate to you you know, it is a core of a lot of things.","width":433},{"text":"Most of the examples I have given are, it is a core of understanding high level cognition In general I like to think about this as knowledge intensive instances, because all the tasks I have given you, you need a lot of knowledge in order to be able to think about them, make predictions.","width":1264},{"text":"It's at a core of building adaptive intelligence systems, whatever definition you want to put into this.","width":464},{"text":"In general dealing with messy, real world data.","width":317},{"text":"Which is really what a lot of computer science and many other applications are doing today.","width":495},{"text":"We are done dealing with good clean data people have organized for class and we are trying to deal with more naturally occuring data of different sorts and this is where machine learning is going to be easy and necessary Has to do with analytics, which became some kind of code buzz word for...","width":1360},{"text":"use a lot of data and tell me about this data.","width":173},{"text":"So it tells a lot of goals, for knowledge acquisition to the integration of various knowledge sources, to ensure adaptations, to supportin decision making.","width":1118},{"text":"In all this context you can find needs for machine learning.","width":315},{"text":"So what is machine learning?","width":149},{"text":"I am not gonna wait for a definition from you, cause we will have 250 definitions.","width":396},{"text":"I am going to use a definition that Hebb Simon who was a computer scientist, economist given probably 50 years ago or so He said, learning denotes changes in the system that are adaptive in the sense that they enable the system to do the task or tasks drawn from the same population more efficiently and more effectively the next time.","width":2098}],
//   [{"text":"which is, if you think about it, a pretty good definition of what learning is.","width":402},{"text":"And I wanna emphasize one thing here, 'drawn from the sample population'.","width":277},{"text":"This is actually the key.","width":124},{"text":"If you haven't thought about it yet, think about it, and you will have to think about it in the course of this semester.","width":349},{"text":"Because this is one key starting point for us, as computer scientists, to think about this problem, if we wanna think about machine learning in any formal way.","width":760},{"text":"So that we can write programs.","width":142},{"text":"So that we can analyze programs.","width":126},{"text":"So that we can say something about the performance of a machine learning program.","width":297},{"text":"Can we say whether it's gonna do well on previously unseen examples?","width":296},{"text":"Can we say something about how well ti's gonna do?","width":223},{"text":"Can we give some guarantees on performance, which is the job we have as computer scientists?","width":446},{"text":"It all hinges on this part of this statement, 'drawn from the same population'.","width":372},{"text":"If we don't make this kind of assumptions, or similar assumptions, there's nothing we can do.","width":430},{"text":"And this is gonna be a core component of what we are gonna do in this class.","width":264},{"text":"We're gonna try to think about why does it work, or when does it work, and when it doesn't.","width":393},{"text":"So I wanna actually augment the definition and I wanna think about machine learning mostly as the ability to perform a task in a situation which has never been encountered before.","width":840},{"text":"So to me, learning is really generalization, or abstraction if you want.","width":256},{"text":"If you think about it, all the examples that I've given before were example that for vast majority of you were new.","width":535},{"text":"Nevertheless, it didn't bothered you.","width":125},{"text":"You could do the prediction quite easily.","width":166},{"text":"In fact, almost any situation that you encounter in life at the very fine level of representation, it's new.","width":612},{"text":"When you are listening to what I'm saying now, behind my fine accent, you have no problem of understanding it even though it's a new signal for you.","width":730},{"text":"When you look around, every time yo see a completely different image, nevertheless, you can interpret it.","width":531},{"text":"So the key question is how do you do the subtraction.","width":291},{"text":"This is really what machine learning is about, so don't lose track of it even though we're gonna force you to lose track of it because it's gonna be more technical, but this is really what we are aiming at.","width":762},{"text":"To figure out how to generalize.","width":248},{"text":"And again, you can think about the spam filter, whenever you see a spam message, you know it's spam even though you've never seen it before.","width":775},{"text":"And you won't just, in this case, do the simple thing- decide yes or no in this case.","width":432},{"text":"So this learner able or at least attempts to classify items it has never seen before.","width":600},{"text":"And you gave a lot of ideas on what does it mean that it is able to do it.","width":323},{"text":"And what should we look at in this representation, and this is the key.","width":317},{"text":"In what sense is this message similar to others and different from others that you have seen before.","width":538},{"text":"And if you are able to figure out this representational issues, then you have a chance of writing a program that will predict correctly on this.","width":976},{"text":"If you wanna think about learning as generalization, you can dream up a lot of examples and I'm just gonna run through this example very quickly, it happens everywhere.","width":633},{"text":"Classification problem like medical diagnosis.","width":194},{"text":"Credit card applications, that for years already, are using machine learning technology.","width":366},{"text":"Or recognizing hand writing letters.","width":184},{"text":"And many other things.","width":112},{"text":"You can see it in other more involved scenarios, like planning and acting, navigation, self-driving cars, game-playing.","width":617},{"text":"You can see it in skills, how to balance a ball, how to play tennis.","width":345},{"text":"These are all learning activities that rely on the ability to generalize from previous situation to future unseen situations.","width":1009},{"text":"And, of course, common sense reasoning, whatever that means, for example things that dictate natural language interactions, scene interpretations, and other things.","width":766},{"text":"So really I wanna highlight the underlying all these abilities is really the basic ability to perform a task in a situation which has never been encountered before.","width":1024},{"text":"And while this class is about algorithms, so we are gonna talk a lot about machine learning algorithms, I want you to keep thinking about this notion of representation that is as important and as crucial to the ability of machine learning programs to work as the algorithm.","width":1502},{"text":"And in fact, perhaps even more important.","width":167},{"text":"That does not mean that we are gonna spend more time on representation than on algorithms, the reason is that we have less to say about representation than on algorithms.","width":734},{"text":"But it's really a crucial thing.","width":195},{"text":"So why do we wanna study learning?","width":169},{"text":"A lot of opportunities.","width":264}],
//   [{"text":"Basically you can think about a quipping computer system with new capabilities, things that we cannot program conventionally.","width":621},{"text":"Programs that cannot automatically adapt, customize themselves.","width":406},{"text":"too many things.","width":110},{"text":"And of course discovering things that are out there in the data and wanna be able to do this automatically.","width":530},{"text":"So you can dream up many many applications, I ma not gonna spend a lot of time.","width":410},{"text":"But there are applications in the area of you know computer systems with new capabilities, you can think about applications with respect to cognition.","width":619},{"text":"Human biological learning and even think about understanding teaching better.","width":369},{"text":"Different notion of the word learning perhaps, still highly related and the key thing the reason why so many of you are here because the time is right.","width":691},{"text":"People have realized in the last few years that actually we can do something with machine learning.","width":491},{"text":"There are initial algorithms and theory in place as you will see as this might be a little disappointing to some of you in the course of this semester, most of the algorithms, mos tof the key algorithmic ideas that we are going to cover in the class were out there in the 60's.","width":1145},{"text":"Really the basic algorithmic ideas were out there.","width":211},{"text":"I am going to point this out.","width":148},{"text":"What has changed is that we have developed theory and we understand what they do, when they do and how they work and how they don't work pretty well now.","width":504},{"text":"And the theory has been developed say in the last 20-30 years.","width":336},{"text":"We have a lot more data and we have a lot more computing power.","width":589},{"text":"Both CPU and other versions and memory, and this actually has made huge impact on our ability to use algorithmic ideas with a few new tricks, but minor tricks.","width":986},{"text":"To get machine learning to actually do something significant.","width":312},{"text":"Not to say we know everything, there is a lot of things we still don't know in machine learning, and hopefully the media hasn't convinced you yet, someone has solved all the problems, they haven't.","width":1333},{"text":"We can do a lot things.","width":134},{"text":"As I say it is also necessary, things that computer science and other areas are trying to do today, we realized that we cannot do them without machine learning.","width":718},{"text":"To me this is going to be key.","width":154},{"text":"Almost everything we are gonna do in the future, dealing with real world messy data will necessitate using machine learning techniques, influenced techniques.","width":922},{"text":"So in this sense you are in the right place.","width":321},{"text":"As I said many unresolved issues, so something for you to work on!","width":664},{"text":"I want to move on to what is this class about and what I think you should know, before that machine learning touches a lot of areas.","width":696},{"text":"Both at the application level and at the technique level, it makes use of probability, statistics, linear algebra, statistics again, theory of computation and so on.","width":930},{"text":"I expect you know some of it, or been exposed to some of it before you came here.","width":605},{"text":"One important thing again to that has perhaps to do something with the expectation you have from this class is that it is an extremely active field.","width":746},{"text":"Hundreds of papers being written as we speak.","width":431}],
//   [{"text":"So welcome to CS 446 and Ok so most of you were here last time, so you know everything.","width":694},{"text":"I am going to repeat few of the administrating things, that you need to know.","width":561},{"text":"Basically we talked a little bit about last time about example of what machine learning is and something about what you need to know.","width":608},{"text":"HW 0 is on the web.","width":204},{"text":"Hopefully many of you have looked at it already so you have a better idea of what you think you know.","width":592},{"text":"Note that HW 1 will be made available next week, HW 0 is just for your pleasure.","width":528},{"text":"HW 1 you will have to submit, in fact on monday, if everything works out we will give you a quiz, it will be a short quiz, it won't take more than a few minutes of your time, we are gonna do this hopefully once a week, and the idea in this case is going to be to make sure you have looked and tried to work on HW 0.","width":1872},{"text":"I talked also about, I want you to talk and I'll give you opportunities today and talked about few of the policies, unless there are any questions.","width":1018},{"text":"Any questions?","width":416},{"text":"Ok so everything is going to be on the webpage, all this information is already in the information page.","width":481},{"text":"If it's not there and there are questions please let us know.","width":223},{"text":"But we are going to try and keep the webpage upto date and we are also going to supply information on piazza.","width":483},{"text":"Here is information about the team.","width":236},{"text":"We have 5 TA's which are sitting somewhere in the back there.","width":502},{"text":"They are going to have office hours, starting from next week and discussion sessions from either next week or two weeks from now.","width":895},{"text":"The week after next OK.","width":126},{"text":"The times are gonna change slightly but basically, you'll see the information.","width":578},{"text":"As I said everything is on the web.","width":187},{"text":"2 important comments- we are video taping it, twice.","width":398},{"text":"We will need volunteers or chore we haven't decided yet completely, we need people to caption the videos because it is mandatory that the videos be captioned.","width":894},{"text":"I am also looking for people to scribe the class.","width":161},{"text":"I got a few volunteers, the more the better.","width":196},{"text":"Not volunteers actually, I am going to pay big money (laughter) if you are doing this job that is.","width":690},{"text":"Announcements- that is important.","width":211},{"text":"If you were asleep so far, no classes next week but as I said the sections will start a week after, but there is quiz next week and the first homework will be made available to you next week.","width":984},{"text":"So don't go to sleep completely, you'll have to start working.","width":493},{"text":"Questions before we start?","width":236},{"text":"So last time, I gave you this puzzle, this learning puzzle.the badges game.","width":678},{"text":"I talked a little bit about it.","width":287},{"text":"Mostly I wanted you to think about it, in your spare time.","width":373},{"text":"It's really a machine learning puzzle in the sense that this is, an example of the key learning protocol that we are going to study this semester called supervised learning.","width":691},{"text":"What you got there, is a bunch of examples, some of them were labelled positive examples and some of them were labelled negative example.","width":773},{"text":"This is really something that happened if you read what is written either there or on the web, everything by the way is available also on the web.","width":570},{"text":"the example in the story, this is something that happened a little bit more than 20 years ago.","width":296},{"text":"At machine learning conference in '94.","width":240},{"text":"I was a graduate student, so you can see my name on the list.","width":201},{"text":"these are the names of the people who registered to machine learning at that time, it was a small conference and you could see that I was a positive example.","width":827},{"text":"So, people that organised the conference gave a learning puzzle, they basically on the badge for each person that registered they put a plus+ or minus-.","width":775},{"text":"You are either a positive example or a negative example and the challenge for the participants was to figure out what is the hidden function that governs these labels.","width":711},{"text":"That's the question that I ask you, you know that last time, a few people already told me that they know so I don't want to know the answer yet, but who knows the answer.","width":804},{"text":"Ok somewhere over there, last time said it they know and gave me a reason for why they think they know.","width":517},{"text":"You want to repeat.","width":85},{"text":"*Repeat the same thing I said?* Yes repeat what you said last time.","width":339}],
//   [{"text":"Okay so he knows the answer and his justification for why he is sure that this is the right answer is that his answer is consistent on the paper I gave.","width":1035},{"text":"So the first thing I want to ask is are you happy with this justification?","width":448},{"text":"SO I see some yeses, this is yes.","width":238},{"text":"Many of you are happy.","width":268},{"text":"Can you give me a better justification.","width":219},{"text":"(Student: There could be some overfitting where he is like taking all the cases and label them correctly but may not work on the next example).","width":1185},{"text":"Okay so that is why you are not happy.","width":185},{"text":"So I'm going to say differently what you said.","width":216},{"text":"Can you give me this, this is what you said.","width":212},{"text":"SO this function is a function right?","width":256},{"text":"It is a piece of paper.","width":59},{"text":"It is a function because it maps names to pluses or minuses.","width":257},{"text":"This function is also consistent with all the information.","width":386},{"text":"And this function is not that informative because it doesn't know whether you are a positive example or not.","width":571},{"text":"Now I bet you know whether it is a positive example or not but your justification wasn't convincing enough.","width":604},{"text":"You understand why I'm not convinced by the justification.","width":263},{"text":"Because here is a simple function just write everything down.","width":202},{"text":"It is also doing it.","width":158},{"text":"So give me a better justification.","width":167},{"text":"People that know the answer.","width":82},{"text":"Why are you sure that this is the right answer.","width":771},{"text":"Why do you think, why did you stop searching?","width":340},{"text":"You thought a little bit about it, oh wow it's this.","width":1926},{"text":"Yeah.","width":42},{"text":"[Gibberish].","width":245},{"text":"Okay, Okay.","width":158},{"text":"Are you happier with this justification?","width":232},{"text":"Why?","width":330},{"text":"Yeah.","width":52},{"text":"And you showed moreover that this hypothesis that he has has some predictive power.","width":817},{"text":"Okay.","width":299},{"text":"I'm looking for an even stronger justification.","width":223},{"text":"Why are you sure that or pretty sure that you're answer is good.","width":933},{"text":"Let's try again.","width":43},{"text":"Who knows the answer?","width":302},{"text":"More people than before.","width":108},{"text":"I'm not going to pick on you if that's the worry.","width":594},{"text":"Who knows the answer.","width":197},{"text":"Let's put it up there and you will look at it again so this is (let's see if it works) so here are some examples.","width":937},{"text":"Hopefully you can read it.","width":115},{"text":"Or in fact I have it also here.","width":141},{"text":"So let's.","width":97},{"text":"Here are some of the examples.","width":283},{"text":"Test your answer on this.","width":416},{"text":"Is there a unique answer you think?","width":769}],
//   [{"text":"So let's go back to the people that volunteered before, give me a better idea.","width":808},{"text":"Why did you stop searching for a better answer.","width":285},{"text":"[student:'because I didn't find a better answer'.] But in what sense your answer was good?","width":1071},{"text":"Ok, let me give you another hint, let's use this artifact again.","width":252},{"text":"Yeah, suggestion?","width":94},{"text":"[Student:' if you can verify anyone with the same rule.","width":414},{"text":"That would be the right rule.'] That's good, but how long is this answer?","width":614},{"text":"this is also an answer, how long is it?","width":317},{"text":"like two hundred names time ten characters each.","width":275},{"text":"two thousand characters, plus or minus.","width":293},{"text":"How long is your answer?","width":547},{"text":"How many characters do you need to write it down?","width":460},{"text":"[gibberish] Yes?","width":126},{"text":"[Student:' I don't have and answer, but the rule should be very simple, much less complicated.'] It's a very very simple rule, yes.","width":1262},{"text":"[Student:'I think it's just one character or two.'] I think it's not enough, but we can check your answer later.","width":693},{"text":"Your encoding scheme's too tight.","width":114},{"text":"But yes, the answer is very very short.","width":213},{"text":"And that's much more convincing for your than a piece of paper full of characters.","width":366},{"text":"And that's a convincing argument, and I bet that was the reason you stop even though you didn't articulate this.","width":479},{"text":"So before we get into the answers and so on, here's just two examples.","width":455},{"text":"Which problem was easier?","width":123},{"text":"I know that you didn't do your homework.","width":124},{"text":"And probably this is a good lesson, and next time please do your homework.","width":315},{"text":"Because all of you can solve this puzzle, some of you will solve it within a minute, some you of you may solve it in ten minutes.","width":440},{"text":"But it doesn't matter.","width":70},{"text":"Spend some time and think about it, because this is really the core of machine learning from many perspective.","width":494},{"text":"So which problem was easier, the problem I gave you?","width":223},{"text":"or the one that was given to the people participating the conference.","width":420},{"text":"[Student: 'the one you gave us, because we had all the data.'] Ok, one reason you have all the data on one piece of paper, and they had to look around, look at badges of people, maybe they didn't even see all the data because they can't meet all the people.","width":1116},{"text":"[Student:'Ours.","width":25},{"text":"Because we know it's only about name.'] Right, so that's another reason.","width":290},{"text":"I gave you the data and I told you it's here, and i gave you only the names.","width":335},{"text":"People in the conference could think about the height of the person, or the university they come from, or their shoes size or something like that.","width":714},{"text":"And for your I restricted the scope of what properties of the names dictate plus or minus.","width":614},{"text":"Much much easier problem.","width":123},{"text":"This is something you always wanna have in a learning problem.","width":350},{"text":"So much easier problem.","width":366},{"text":"So this is the story.","width":156},{"text":"So let's get an answer.","width":301},{"text":"Who wants to give me the answer?","width":149},{"text":"Ok, you were first.","width":83},{"text":"[Student:'If the second letter is a vowel, that doesn't include 'y', y is not a vowel.'] Second letter of?","width":562},{"text":"[Student:'Second letter of first name.'] Second letter of the first name is?","width":303},{"text":"Is a vowel.","width":85},{"text":"That makes this positive example, otherwise it's a negative example.","width":425},{"text":"Ok, so what is a vowel?","width":678}],
//   [{"text":"Did you understand the definition of the function?","width":401},{"text":"What is a vowel?","width":248},{"text":"Yeah.","width":2},{"text":"It is the set of letters a, e, i, o u.","width":202},{"text":"But it is not part of the input, I gave you just a bunch of characters and a class+/-, I did not say vowel or not vowel.","width":527},{"text":"It is not part of the input.","width":315},{"text":"So, this is what?","width":393},{"text":"Where did you get vowel from?","width":137},{"text":"All of you agreed on this.","width":89},{"text":"All of you know what vowel is.","width":347},{"text":"So, vowel is a set of 5 characters.","width":138},{"text":"If instead of vowel I would say, the second letter of the first name is one of x, y, z, c, k would it be as easy a concept?","width":1304},{"text":"Yes/no?","width":44},{"text":"Who says yes?","width":248},{"text":"Who says no?","width":153},{"text":"Why not?","width":190},{"text":"Yeah.","width":494},{"text":"So for you it's not as simple because you have this notion of vowel and because you can write this function using vowel, it is very simple definition for you and you buy it.","width":847},{"text":"If it were a longer definition you would not buy it and you will look for something more compact.","width":329},{"text":"What about your program, that you write?","width":255},{"text":"Is this the same for the program?","width":227},{"text":"Think about writing a program for this, which was another question that I asked last time.","width":317},{"text":"I don't know if you thought about how to write a program that will figure it out.","width":284},{"text":"But let's imagine that you know how to write a program.","width":366},{"text":"Is it as difficult for the program whether it is a vowel in the second place of the first name or one of x, y, z, c, k?","width":605},{"text":"What do you think?","width":350},{"text":"Yeah.","width":525},{"text":"So what does a program like this need to do?","width":242},{"text":"How would it work?","width":391},{"text":"Imagine you want to write a program that learns this function.","width":226},{"text":"What would you do?","width":199},{"text":"Anyone thought about it?","width":647},{"text":"Yeah.","width":13},{"text":"So for each character in the input you will decide whether it appears in positive examples or negative examples.","width":794},{"text":"Okay.","width":80},{"text":"That makes some assumptions.","width":105},{"text":"What assumption have you made in suggesting that this is the program?","width":427},{"text":"That only the characters matter and the position does not matter and in fact we know that this is not true already.","width":581},{"text":"More assumptions you have made?","width":320},{"text":"Right, so maybe it is just based on length or the first token or second token or the sum of the tokens, could be other things, right?","width":585},{"text":"So we need to improve this program.","width":151},{"text":"Any suggestions?","width":587},{"text":"But that is actually a good start, right?","width":98},{"text":"So basically what you have done is said: I'm going to hypothesize some properties of the name.","width":427},{"text":"For example, does it have 'a'?","width":96},{"text":"Does it have 'b'?","width":43},{"text":"Does it have 'c'?","width":82},{"text":"Maybe along with position: does it have 'a' in the first place, the second place, the third place?","width":322},{"text":"Does it have 'c' in the third place and so on.","width":132},{"text":"And, eventually I am going to start searching and find something.","width":342},{"text":"Perhaps.","width":151},{"text":"Any other suggestions on how to go about doing it?","width":412}],
//   [{"text":"Okay, so this will be your second problem set.","width":305},{"text":"By that time you'll know how to do it.","width":179},{"text":"So let's see...","width":45},{"text":"so, we talked a little bit about whether your hypothesis has a prediction power or not.","width":525},{"text":"So if it's just a list, a piece of paper here, it's correct, but it doesn't give me anything.","width":413},{"text":"It doesn't have any predictive power, and you won't function really.","width":263},{"text":"We talked a little bit about the representation.","width":128},{"text":"You realized that the people in the conference had a much larger set of potential representations to choose from than you guys had, and even for you, it's not completely clear what are the components of this representation.","width":1046},{"text":"Are they gonna be the characters?","width":107},{"text":"Are they gonna be the character and positions?","width":165},{"text":"Is it gonna be the length?","width":114},{"text":"Is it gonna be maybe the position in the list?","width":206},{"text":"I'm not sure what could be there, but the representation is crucial because if you don't choose the key components, you'll never be able to do it.","width":694},{"text":"Another important thing is this issue of the vowel here.","width":320},{"text":"For you, the fact that the second character was a vowel was actually a key simplifying assumption.","width":542},{"text":"You have to think about whether this is true for the program that you will write.","width":324},{"text":"And to what extent, if you tell the program, the concept of a vowel, will it simplify its learning or not.","width":633},{"text":"What do you think?","width":138},{"text":"Let's assume that your program also knows what a vowel is.","width":283},{"text":"Does it matter?","width":335},{"text":"We'll take a vote again.","width":117},{"text":"Who thinks it's gonna matter?","width":160},{"text":"How would you use it?","width":248},{"text":"Yeah?","width":129},{"text":"[student answers, inaudible] If you know that the concept of a vowel exists, and it's there, it puts together, potentially, a lot of things that, otherwise, you would have to search.","width":1179},{"text":"Maybe you would have to search all subsets of some size; whether an element of this subset is in the second place or the third place or the fourth place.","width":681},{"text":"If I give you some interesting subsets, that have some unique properties, maybe you will only search for them.","width":411},{"text":"So there are a lot of things that have to do with it, but this also means that some of the learning that you did, in order to figure out this function, did not happen once, or after I gave you this piece of paper.","width":952},{"text":"Some of it happened before, where you decided that, you know, some concept or subconcept could be important.","width":458},{"text":"Vowel is important, you learned that years ago.","width":247},{"text":"So there are a lot of issues here even though we did not get to any algorithm yet.","width":352},{"text":"I don't know how you did it, and you don't know what algorithms could allow us to do it.","width":390},{"text":"In fact, a lot of algorithms will do it, and we will see that a little bit later.","width":359},{"text":"Okay, so I'm putting this again, with the spam example that I gave last time to remind you that even there, in exactly the same way, had I told you some aspects of this email message that are important, and some that are unimportant, to determine whether it is spam or not, this will simplify your problem a lot.","width":1382},{"text":"It's kind of like telling you that it's all in the name, rather than in many other issues.","width":431},{"text":"So this is...","width":122},{"text":"kind of focusing on the right part of the input, as a big step, first.","width":413},{"text":"Any questions on this?","width":400},{"text":"Okay, so the game that we've played here is the game of supervised learning, which is the key protocol, the key learning protocol that we're gonna talk about.","width":591},{"text":"So, we really care about systems that apply function 'f' to input items 'x', in this case, strings of characters, and return some output, y, which is f(x).","width":877},{"text":"So, we're gonna have some input, we're gonna have some output, and we're gonna ask the question of what y can be there?","width":456},{"text":"What f(x) can be there?","width":205},{"text":"So, a supervised machine learning algorithm, or supervised machine learning, deals with systems where f(x) is learned from examples.","width":592},{"text":"You give me a bunch of 'x's and 'y's, and I wanna infer the same.","width":366},{"text":"So why do we need this learning?","width":154},{"text":"The key reason is because we wanna apply it when we don't know what f is.","width":478}],
//   [{"text":"And that always happens when you sent me an email after the lecture last time some percentage of the email you sent was wrong.","width":771},{"text":"So I asked to send a fixed string.","width":193},{"text":"x% of you did not send this string.","width":140},{"text":"So there is always noise in the input.","width":252},{"text":"That's a good thing to know about.","width":135},{"text":"That reminds me that I must say something about registration.","width":200},{"text":"I am sure you are all.","width":106},{"text":"There is a waiting list.","width":108},{"text":"It is a long waiting list.","width":136},{"text":"Some of you already got in from the waiting list and my assumption is that by the end of today or tomorrow at least twenty more will get in.","width":764},{"text":"I am not as optimistic as I was on Tuesday morning that all the waiting list will get in because it is long.","width":603},{"text":"But, I believe a large fraction of it will get in.","width":214},{"text":"So just that you know, but as I said there is a long time, a few weeks by which you don't have to really be stressed about it.","width":1051},{"text":"What I do request is that if you have decided that you are not going to take the class please leave the class as soon as you can.","width":625},{"text":"If you want to drop off the mailing list, let me know as soon as you can that you don't want to be on the waiting list.","width":430},{"text":"Okay, let's continue then.","width":180},{"text":"We talked a little bit about how we train.","width":216},{"text":"Then we have to test.","width":93},{"text":"We have a hypothesis, we want to test.","width":110},{"text":"So typically are going to reserve some labelled data for testing; so this is labelled data that no one saw before.","width":459},{"text":"We are going to show the raw test data, only the x's to the learning algorithm look at the y labels that are true and compare them to what the algorithm predicts, the g(x)'s.","width":1107},{"text":"So this could be one way to test it.","width":282},{"text":"If g(x1) equal y1 we are happy or else we are not happy and we are going to count the mistakes.","width":494},{"text":"Can you imagine other ways to evaluate?","width":621},{"text":"So the way I suggest, yeah.","width":438},{"text":"Say it again I didn't get it.","width":640},{"text":"That you can evaluate g on all the x's.","width":248},{"text":"I am assuming here that g can run on all the domain and x's come from this domain so I can always evaluate g.","width":905},{"text":"But how do I know whether it is good or not or how do I judge myself whether it is good or not.","width":568},{"text":"You can do in multiple ways: you can count how many times g(xi) is yi or if I have some notion of distance; let's say it is not just boolean zero or one but rather it is a real number from zero to one, maybe I can measure error: how far is g(x) from y.","width":1774},{"text":"And there could be some other ways of doing this.","width":157},{"text":"So correctness is not just the only way, we are going to see other ways.","width":412},{"text":"So, hopefully with this example some things are clear.","width":414},{"text":"This is just going back to what I showed last time: what we are going to be doing in this class I am not going to dwell on it now we are going to start talking about algorithms.","width":694},{"text":"In fact, hopefully, start talking about algorithms today and give some real examples of learning.","width":539},{"text":"So I wanna start with some terminology though.","width":211},{"text":"What is the game we are playing here: we are giving a set of examples x with label f(x) for some unknown function.","width":595},{"text":"We want to find a good approximation of the function f.","width":243},{"text":"Maybe identical to f maybe a good approximation we will have to define that.","width":289},{"text":"So x gives us some representation of the input and this is an important process here.","width":573},{"text":"The process of taking the input could be an image, a sentence, could be document could be a string.","width":486}],
//   [{"text":"And mapping between input and the learning algorithm it takes.","width":347},{"text":"It is said, a key step in the machine learning process is called feature extraction process.","width":478},{"text":"It is how to both computationally and conceptually....","width":253},{"text":"In fact, if you think about the lifetime of machine learning algorithm, you will realize, once you start using it, most of the time, both CPU time and your brain time is being spanned on the future extraction process, not on the learning itself.","width":1250},{"text":"Moreover, it is even the ...we deal with for this, but very important.","width":374},{"text":"So this x, once represented.","width":236},{"text":"The input can be represented as a boolean vector, 0, 1, real value vector, and so on.","width":527},{"text":"And this could be binary classification, could be multiclass classification, multiple values.","width":556},{"text":"I wanna know what is the topic of this document.","width":133},{"text":"Is it news, finance, sport?","width":167},{"text":"Or could be just real value number.","width":218},{"text":"A lot of examples that you can think of, listing if you hear, can be written, or can be reduced into this multicalss classification problem.","width":1020},{"text":"Use an input, classify it into one of k values, even when the problem seems a lot more complicated, you can reduce it to this.","width":588},{"text":"Spend more time on examples.","width":244},{"text":"I wanna start by mentioning some of cases in machine learning.","width":384},{"text":"So this is perhaps the most important slide, not only today in the class, because it really gives you the process.","width":525},{"text":"While you always think about the last component in this process.","width":280},{"text":"Really the process has 3 steps.","width":204},{"text":"The first one is modeling.","width":108},{"text":"How do we take an application and formulate it as a machine learning problem?","width":289},{"text":"How do we present the data?","width":118},{"text":"What learning protocol that is valuable to us?","width":195},{"text":"And what make sense in this scenario?","width":345},{"text":"Representation.","width":108},{"text":"What function should use as candidates for learning class here?","width":580},{"text":"Not every function can do it.","width":270},{"text":"And this decision is gonna have a lot of significance in whether we will success or not.","width":571},{"text":"How do we make the data, the old data into X space, the instance space?","width":307},{"text":"And the question of course is: is there any general approach?","width":313},{"text":"can we say something rigorously about this thing?","width":191},{"text":"And finally algorithm.","width":124},{"text":"So of course we will spend a lot of time studying the algorithm and analyzing the algorithm.","width":366},{"text":"But the important thing for perceptive is gonna be: how do we know whether one algorithm is better than the other?","width":376},{"text":"Can we say something about the algorithm?","width":183},{"text":"How do we define success?","width":135},{"text":"I already hinted about it few slides ago.","width":167},{"text":"So it is not necessary that there is one matrix by which we should decide whether you've done or not.","width":437},{"text":"That could be multiple things.","width":158},{"text":"As we said earlier today and last time.","width":259},{"text":"The key thing about learning is not how well it is doing in this data that they see here, the piece of paper that they give you.","width":502},{"text":"But rather how well it is gonna do on data that you've never seen before.","width":383},{"text":"Can we generalize for it?","width":86},{"text":"So this generalization ability vs fitting is gonna be important.","width":409},{"text":"And of course the computational problem is gonna be an issue.","width":333},{"text":"Does computer scientist want to know can we compute this efficiently?","width":305},{"text":"Is the learning problem tractable or intractable?","width":259},{"text":"We will be able to say things about all the cause in this class.","width":458},{"text":"So I am asking these questions in a little bit more specific with the right terminology.","width":561},{"text":"What is the instance space?","width":68},{"text":"What is x?","width":73},{"text":"What kind of features are we gonna use?","width":146},{"text":"What is a label space?","width":112},{"text":"What kind of learning task is it?","width":107},{"text":"Is it gonna be mapping things into binary things?","width":221},{"text":"I am gonna choose 3 lables.","width":257},{"text":"What is our hypothesis space?","width":179},{"text":"What kind of functions are we gonna learning?","width":165},{"text":"Are there some function spaces that are easier?","width":309}],
//   [{"text":"Let's check, if x4 is 1, and x2 is off, it's a positive example.","width":757},{"text":"Otherwise, this is an another positive example also.","width":282},{"text":"x4 is 1, x2 is not.","width":230},{"text":"And all the rest are zero.","width":266},{"text":"So?","width":184},{"text":"That's a function, is that the function?","width":354},{"text":"Who says yes?","width":280},{"text":"Wake up.","width":368},{"text":"Ok, so you have another suggestion.","width":350},{"text":"Ok, so x1 and x4, ok.","width":503},{"text":"That's also [gibberish].","width":143},{"text":"Is it the same function?","width":191},{"text":"In more sense, it's not the same function.","width":285},{"text":"yea?","width":694},{"text":"I want to say what you said in a more general way.","width":349},{"text":"these are not the same function, even though they agree on the seven examples.","width":365},{"text":"but they are not the same function in the sense of there exist at least one instance x1, x2, x3, x4 on which they differ.","width":615},{"text":"so which one should I use?","width":118},{"text":"This one or this one?","width":209},{"text":"so what is the problem we are having here?","width":223},{"text":"we already have in the minutes, two possible outcomes of your learning algorithms.","width":408},{"text":"I took the learning algorithm you use.","width":87},{"text":"so here is the whole picture here.","width":126},{"text":"on 4 inputs, I can have 16 instances.","width":324},{"text":"I just list all the 16 instances.","width":140},{"text":"everyone agrees that I can have 16 instances.","width":168},{"text":"right?","width":83},{"text":"how many functions are there on 4 inputs?","width":970},{"text":"yea?","width":223},{"text":"2 to the 2 to the 4.","width":71},{"text":"2 to the 16.","width":108},{"text":"right?","width":23},{"text":"There are 2^16 possible functions, what is that?","width":275},{"text":"So, again, each one of these instances, each one of these 16 instances here, I can either put 0 or 1, two options, against each one of those.","width":753},{"text":"right?","width":82},{"text":"so it's 2* 2* 2* 2, 2^16 is the number of functions.","width":319},{"text":"If I change one of these labels.","width":123},{"text":"I got a different function, right?","width":133},{"text":"because at this point, the functions disagree.","width":269},{"text":"so I have a lot of functions.","width":79},{"text":"65 thousands functions.","width":107},{"text":"so you give me one, you give me one, how many functions are actually possible here?","width":521},{"text":"given these learning problem.","width":204},{"text":"just a little bit less because I already give you the examples.","width":470},{"text":"I told you this seven examples.","width":192},{"text":"This example must be 0, This example must be 1, This must be 0 and so on.","width":331},{"text":"I left only nine examples open, so how many functions?","width":436},{"text":"2^9, still a large number of functions.","width":218},{"text":"and you give me two, how do I know what to do here.","width":506},{"text":"so the first question you should ask yourself is learning even possible, right?","width":238},{"text":"I give you few examples, there are so many options here.","width":394},{"text":"yea?","width":140},{"text":"how much data you want?","width":231},{"text":"all of it.","width":31},{"text":"ok, if you do all of it, then the table is the function, but um..that's gonna be the only function.","width":647},{"text":"but that's not realistic and not interesting situation.","width":206},{"text":"so if you instead of 4 bits, there will be a hundred bits, or thousands bits, or millions bits, you see that you never see all the data.","width":642},{"text":"and the number of functions, is gonna go doubly exponentially with this.","width":412},{"text":"so is learning possible?","width":283}],
//   [{"text":"Okay, I have a lot of sentences, I took all of Wikipedia.","width":255},{"text":"That's my input.","width":82},{"text":"But i still need to define the domain, right?","width":132},{"text":"So, the...","width":190},{"text":"Okay, what best representations, you're suggesting, let's make it a little bit more concrete.","width":358},{"text":"Here's what I am going to do.","width":119},{"text":"For each word in English, I am going to define a boolean feature, boolean, zero, one, x (and) w is going to be one if and only if w is in the sentence, otherwise it's zero.","width":809},{"text":"Okay?","width":38},{"text":"So these are my features, okay?","width":177},{"text":"That's kind of what you had in mind, that's it.","width":176},{"text":"So, this way, let's assume there are fifty thousand words in English.","width":331},{"text":"So, I am writing a vector of size fifty thousand, and, for each word, let's say the word 'I', is the seventeenth word in the sentence...","width":852},{"text":"in this representation, the fifty thousand dimensions, so in this sentence 'I' is going to be 1.","width":421},{"text":"In other sentences 'I' is going to be 0.","width":122},{"text":"The seventeenth place is going to be 0.","width":150},{"text":"So that's my encoding right?","width":85},{"text":"So I encoded every sentence, I have a method to encode every sentence.","width":458},{"text":"As a bit vector if you want, of dimensionality fifty thousand.","width":361},{"text":"Okay?","width":78},{"text":"That's clear?","width":65},{"text":"So now, I am working in the space 0, 1 to the 50000-th.","width":379},{"text":"Some points are whether, 'wh' points, and some points are 'we' points.","width":326},{"text":"Okay?","width":38},{"text":"So the picture is kind of like this.","width":388},{"text":"Okay?","width":35},{"text":"That's the picture.","width":68},{"text":"I have a space, that's not two-dimension as you've seen here, it's fifty-thousand dimentional, and it has some red points and some purple points.","width":594},{"text":"That's the learning problem, okay?","width":151},{"text":"I did not talk about learning protocol here.","width":190},{"text":"But we can talk about learning protocol in a second, so what is, learning protocol?","width":398},{"text":"Do you want to think about it as a supervised learning algorithm?","width":688},{"text":"Where are you going to get the labels from?","width":232},{"text":"Where?","width":280},{"text":"The sentences?","width":195},{"text":"Okay, so that's actually the interesting thing.","width":130},{"text":"That is why I asked this question, so, I am going to use a supervised learning algorithm here.","width":372},{"text":"Because we don't know better, that's what we know how to do.","width":237},{"text":"You will see it later on.","width":111},{"text":"We only know how to do supervised learning.","width":148},{"text":"What we do know, is sometimes to trick ourselves, into, supervised learning algorithm and this is what we are going to here today.","width":552},{"text":"I am going to take all of wikipedia, for example, as my data, and I am going to make the assumption, that is largely correct, that wikipedia does not have spelling mistakes.","width":686},{"text":"So when I see a sentence with whether 'wh', it is correctly used.","width":342},{"text":"I am going to drop it from there, and take this as a sentence, as positive, for whether 'wh', and negative, for weather 'we'.","width":568},{"text":"I am going to see another sentence that had used weather 'we', take it away, present it to my algorithm as an example that is positive for 'we', negative for 'wh'.","width":785},{"text":"So I have a supervised scenario, even though no one labeled the data for me.","width":457},{"text":"I just made an assumption, and used this as my labelling algorithm, if you want.","width":371},{"text":"And that's an important trick that, in most sophisticated way, is crucial to today's machine learning, because we can never hire enough people to label data.","width":718},{"text":"But, with this simple trick, now what I have is a supervised learning scenario, and we can continue, okay?","width":464},{"text":"So we placed these points here, and now, the goal is to find a function, the separates the data best.","width":601},{"text":"Right?","width":39},{"text":"Clear?","width":108},{"text":"So, here is a function, that separates the data.","width":278},{"text":"Here's another function, that kind of separates the data, another function.","width":272},{"text":"Another one.","width":91},{"text":"Which one do you want to take?","width":233},{"text":"Of these?","width":372},{"text":"You want to take the blue line.","width":131},{"text":"Other suggestions?","width":187},{"text":"The green line, the green curve, let's say.","width":177},{"text":"Why do you want to green?","width":347},{"text":"[Student answering question].","width":312}],
//   [{"text":"OK, so that's Question One.","width":180},{"text":"If you do have additional questions, I'll remind you that there are discussion sessions that start today in the afternoon.","width":525},{"text":"Everyday of the week, there's gonna be one.We actually ask you to split based on the last name.","width":532},{"text":"Please, try to do as we asked unless you really have a problem.","width":459},{"text":"The reason is that we want to kind of maintain uniformity as much as possible.","width":350},{"text":"Question Two, there is nothing to say about it.","width":127},{"text":"We've all we've seen it before.","width":111},{"text":"It was part of Homework Zero.","width":194},{"text":"Question Three is probably is the most challenging one.","width":504},{"text":"Starts here.","width":66},{"text":"And really it's a question about modeling, so we want you to show that you can model the learning problem using a linear program.","width":743},{"text":"I assume that some of you haven't seen linear programs before.","width":243},{"text":"If you haven't, we gave a lot of material, just the definition...","width":267},{"text":"We gave definitions and examples and enough material for you to really understand the definition.","width":531},{"text":"The key challenging part here is modeling.","width":363},{"text":"So, and this actually illustrate some of the difficulties that you'll see in the class, and I'm gonna illustrate at least once later today.","width":579},{"text":"It's really that the ability to abstract and see that this problem can also be viewed as that problem.","width":410},{"text":"So, I think that this is gonna be the one that will take you the most time, so please start, and come to the discussion section to ask questions.","width":839},{"text":"OK, so lets just finish with this here.","width":329},{"text":"Projects, we got a lot of questions about projects.","width":190},{"text":"So, here is basically what's gonna happen.","width":177},{"text":"We would like you to have small groups, two-three, no more than that.","width":380},{"text":"As I said at the beginning, people that are registered for 4 units, 4 hours, %25 of the grade is the project.","width":829},{"text":"For undergrads that cannot register for 4, this will also be %25 of the grade, but I'm gonna include it only if it helps you.","width":774},{"text":"Other than that, given that it's %25 of the grade, it has to be clear that it has to be substantial project.","width":502},{"text":"So, I'm gonna ask you to write a proposal, very short proposal, one page proposal, for what you wanna do, so you can start thinking about it.","width":882},{"text":"There are already instructions on the web that they might change a bit but not significantly, on what..","width":593},{"text":"how should a proposal look like, and there are also some ideas for projects that also might change a little bit.","width":464},{"text":"But you can start thinking about it.","width":293},{"text":"OK, next point.","width":91},{"text":"So, we already had two quizzes.","width":158},{"text":"This is new for this class.","width":95},{"text":"The idea is to force you to think about the material also outside class.","width":449},{"text":"You can see here some statistics, almost all of you did the quiz.","width":277},{"text":"All of you should do the quiz.","width":252},{"text":"I think it's gonna be good for your grade and it's gonna be good for you to think about things.","width":345},{"text":"Then results are good.","width":108},{"text":"So, if you have comments on the quizzes themselves, or on the idea of quizzes, or any other suggestions, please either let us know, me or the TAs, individually or through Piazza.","width":999},{"text":"Important questions?","width":149},{"text":"comments?","width":230},{"text":"Yeah.","width":6},{"text":"So.","width":2},{"text":"I checked backed on Compass when I finished my quiz and when it passed the deadline, but I can see my score but I cannot see[?] questions.","width":698}],
//   [{"text":"Well, now I've point in two dimensions x,x^2.","width":404},{"text":"No more information.","width":209},{"text":"Now I can actually separate it with a linear function.","width":269},{"text":"So that's an important transformation and we do this same thing also in the discrete space.","width":363},{"text":"Uhm.","width":72},{"text":"So, let's assume that I have this function.","width":190},{"text":"It's a DNF.","width":123},{"text":"It's x_1 and x_2 and x_2 or x_2 and x_4 and x_5 or x_1 and x_3 and x_7.","width":499},{"text":"Again, think about the admittance to graduate school.","width":338},{"text":"Uhh.","width":77},{"text":"Examples for this might stand for uh very good GPA, very good school, uh very good advisor.","width":804},{"text":"So you have to have these three or some other conditions.","width":330},{"text":"Uh.","width":67},{"text":"So this is a function that is not linear over these x_i's.","width":267},{"text":"Is that clear?","width":171},{"text":"Not linear over these x_i's.","width":310},{"text":"Unlike the conjunction over disjunction that I showed two slides ago.","width":319},{"text":"But I can make it linear by inventing a new set of features.","width":259},{"text":"What I will do is.","width":161},{"text":"My original feature space is this one.","width":160},{"text":"I'm gonna invent a new space.","width":414},{"text":"I'm gonna take every subset of three x_i's, x_i, x_j, x_k, for all i, j, k and invent a new feature.","width":667},{"text":"I'm gonna call it Y.","width":169},{"text":"How many features will I have now?","width":172},{"text":"How large is gonna be my Y-space?","width":338},{"text":"Again, I'm starting with n features x_i.","width":160},{"text":"I'm taking every triple, and the conjunction of each triple I'm gonna call it y_i.","width":490},{"text":"How many y's will I have about?","width":356},{"text":"[student].","width":60},{"text":"About n-choose-3, so about n cubed.","width":200},{"text":"So I started with n features.","width":141},{"text":"Now I have n-cubed features.","width":96},{"text":"A lot more but still tolerable.","width":155},{"text":"Now in this space, uh so this is the space the y-space is the space of all each y is a conjunction of three x_i's.","width":660},{"text":"In this function, this function in this space this function that looks very complicated x_1 and x_2 and x_4 or something or something now looks like a disjunction.","width":891},{"text":"Alright?","width":41},{"text":"Pretty soon you'll know how to learn it because that's your problem set.","width":241},{"text":"It's a disjunction in a larger space, but this is a linear function, so we've started with something that wasn't a linear function.","width":504},{"text":"Now we've transformed the feature space and uh we can do it.","width":278},{"text":"We can learn with a simple function.","width":112},{"text":"So really it's the same example as this example.","width":331},{"text":"Only that here it was in a continuous space.","width":302},{"text":"Here it's in a discrete space.","width":208},{"text":"Really the same example.","width":153},{"text":"Uh question on this.","width":250},{"text":"Ya?","width":72},{"text":"[student question].","width":400},{"text":"Okay that's a very good question: can I always do this?","width":214},{"text":"Uh.","width":120},{"text":"So the answer is uhm in a finite dimensional space, I can always do this.","width":538},{"text":"Let's think about boolean functions.","width":118},{"text":"I can always invent features that can make it linear.","width":175},{"text":"I could cost me.","width":138},{"text":"Maybe I will have to increase the dimensionality very very significantly.","width":403},{"text":"Let's say rather than a DNF of size with conjunction of size three here, I have conjunction of size n/2.","width":692},{"text":"It's gonna be expensive to do it.","width":104},{"text":"I'll have to find other tricks.","width":82},{"text":"It's always possible to linearize it.","width":147},{"text":"Sometimes you may not want to do it, but if the conjunction is so long, it's going to be difficult to learn either way.","width":522},{"text":"So we're gonna talk about when we can learn things and when we cannot a little big later but at least now I want to get the intuition and I wanna give you a concrete example here of how it will look in a real data, real problem.","width":1036},{"text":"So this is an example that you can access uhm.","width":253},{"text":"It's on my uhh data page.","width":149},{"text":"You can find it there.","width":64},{"text":"It's also on the slides.","width":112},{"text":"And it's a collection of data sets.","width":257},{"text":"Uhh.","width":94},{"text":"For uh a lot of context sensitive spelling correction whether-weather is also somewhere here.","width":507},{"text":"Ya it's here.","width":202},{"text":"Uhm.","width":121},{"text":"So this is how it looks.","width":92},{"text":"So first of all, this are sentences you probably cannot read it.","width":430},{"text":"What's happening.","width":160},{"text":"There's nothing you see.","width":69},{"text":"Ok.","width":98},{"text":"Uhmm.","width":5},{"text":"Let's do this.","width":684}],
//   [{"text":"Okay...","width":179},{"text":"so let me go back, I don't know...","width":181},{"text":"so basically, the link from the slide is this.","width":239},{"text":"It's a collection of a lot of context-sensitive spelling corrections.","width":297},{"text":"This is the 'weather' example.","width":236},{"text":"And what you see here is just the raw sentences where for either 'weather' type one, or 'whether' type two, whatever it is here, I just put brackets around it, so it will be easy for the feature extraction program to do the extraction.","width":1212},{"text":"So that's the data.","width":108},{"text":"Now what do we do with this data?","width":207},{"text":"We use features.","width":177},{"text":"So these are feature definitions.","width":135},{"text":"And let me say something about it.","width":122},{"text":"So the language here is something like this.","width":278},{"text":"So, the features that we use are words, and part of speech tagging.","width":447},{"text":"And then we use the location also.","width":152},{"text":"When we sit on a word, 'weather' type one, say, we look up to two to the left, up to two to the right, and we record what the word is there, and what is the part of speech.","width":849},{"text":"For example, this feature says that one means one to the right of the target word is the word 'that'.","width":617},{"text":"This feature says that the part of speech to the right, this is the one, is a terminal.","width":459},{"text":"This feature says that to the left, minus one, the part of speech tagging just before the target word is a punctuation, and so on.","width":597},{"text":"So minus one says go to the left, one or two say go to the right.","width":389},{"text":"And then we write either p or w to represent the part of speech tag or word.","width":264},{"text":"Now you can see that some of these words are conjunctions.","width":285},{"text":"For example, the word just before is a preposition, the part of speech just before is a preposition, and the part of speech just after is a pronoun.","width":803},{"text":"So that's a conjunction, right?","width":124},{"text":"I already knew that just taking the features, as I said: location, part of speech or word, is not gonna be enough, and so I took conjunctions of size two.","width":708},{"text":"In fact, I think if you go down the list, you will see also conjunction of size three of this.","width":394},{"text":"So, the space, if I were just taking the individual features, it wouldn't work.","width":442},{"text":"Because we take conjunctions, it's actually...","width":173},{"text":"a linear function can do it.","width":87},{"text":"So [mumble], these are a list of features.","width":315},{"text":"The numbers that you see beyond that are really not important for you.","width":268},{"text":"I mean, these are just some statistics that I've taken over the features.","width":211},{"text":"For example, the last two numbers indicate whether this feature was active in the present, 'weather' type one, or how many times this feature was active in the 'weather' type one example, and 'whether' type two example.","width":879},{"text":"And you can see that it's actually quite one-sided on the top features here.","width":378},{"text":"You can guess which is 'weather' type one, and 'whether' type two here.","width":328},{"text":"And this number is just a statistic over the feature.","width":223},{"text":"It's a chai squared statistic.","width":80},{"text":"It really doesn't matter.","width":84},{"text":"It's just used to, in this case, sort the features by expected importance, or something like this.","width":492},{"text":"So that's what you get.","width":130},{"text":"So these are really the same type of example that I just showed, right?","width":218},{"text":"So it's conjunctive features over the inputs that we have.","width":346},{"text":"And the final thing I wanna show is, these are the examples.","width":447},{"text":"Right, so now every sentence becomes a list of active features.","width":513},{"text":"So each feature got a name, got a number- was hashed into a number- the first is actually the label in this.","width":567},{"text":"So you can see that the labels are either one or zero.","width":275},{"text":"These are the two types.","width":121},{"text":"And the rest are features, And one important thing to see is that these are variable size examples.","width":558},{"text":"Not every sentence is gonna be encoded into the same number of features.","width":388},{"text":"And you can see also that the numbers become quite large.","width":183},{"text":"They are going to be eventually thousands of features.","width":259},{"text":"Only very few of them are active in each example.","width":179},{"text":"So that means that computationally, you may want to think carefully about how you represent them.","width":452},{"text":"Questions on this?","width":291},{"text":"So is that clear?","width":43},{"text":"That's the learning problem.","width":69},{"text":"That's really the learning problem.","width":98},{"text":"We started with data, we invented features, our features are conjunctions of primitive features, and then we took each sentence, and coded it.","width":958},{"text":"And this is each line, each row here is one example.","width":490},{"text":"Questions?","width":100},{"text":"Yeah?","width":107},{"text":"[student asks question, inaudible] .", "width":401}],
//   [{"text":"And I'm not gonna prove it, I can give some pointers to where this is shown, but for now, what we're gonna do is we're gonna say, 'okay, we cannot do this, let's simplify the problem.' Instead of minimizing what we really want to minimize, which is the [mumble] number of mistakes, we're gonna invent another loss function that will be an upper bound to the real loss function that we care about, and try to minimize that.","width":1992},{"text":"So, we're gonna move from this nice, discrete problem, loss function that we care about, to something that is related to it.","width":661},{"text":"And typically we're gonna [mumble] try to move to a convex upper bound of the true loss function, and the reason is that computationally, it's gonna be easier to minimize.","width":710},{"text":"Okay, so what are we gonna do?","width":102},{"text":"So, our loss function that measures the penalty that we incur (that the classifier incurs on an example) is gonna be replaced now by some surrogates.","width":834},{"text":"As I said, we start with this, the loss function is zero if we got it right (lost nothing), and one, if we made a mistake, and we're gonna replace it, for example, with this loss function: square loss.","width":1028},{"text":"Again, if f(x)=y, the loss is zero, but if f(x) is not equal to y, we lose something.","width":651},{"text":"And we lose as a function of how far f(x) is from y.","width":234},{"text":"You can invent other loss functions.","width":116},{"text":"So this is this loss function, where the x-axis is f(x)-y, right?","width":393},{"text":"But you can invent others, you can say, 'lets define an input dependent loss'.","width":297},{"text":"So, it's gonna be zero if we are okay, and some f(x) otherwise.","width":423},{"text":"So typically, we wanna move to a continuous convex function, because this is easy to optimize, and here are some common examples that people are using.","width":832},{"text":"I'm not gonna give all the details, but look at it carefully later on.","width":200},{"text":"The function that we mentioned before is this black curve, here.","width":582},{"text":"It's this function, 0-1 Loss, it's called.","width":249},{"text":"Now, everything in this graph, or most of the things in this graph, I'm writing in terms of the product, y*f(x).","width":502},{"text":"Now y*f(x), think about it, y could be+/- 1, f(x) could be any real value.","width":562},{"text":"If f(x) is positive, we assume it's a positive example.","width":263},{"text":"If f(x) is negative, we assume it's a negative example.","width":204},{"text":"So this product, y*f(x), is important to us because if it's positive, it means that the sign of y and f(x) agree; either both of them are positive, we are happy, or both of them are negative, we are also happy.","width":1014},{"text":"So sign(y*f(x)) is an indication of whether we are on the right side, or not.","width":564},{"text":"If the signs of y and f(x) are different, it means that y*f(x) is negative, then the sign is gonna be negative one.","width":541},{"text":"You can check the 0-1 loss here, sign(y*f(x)), if it's one, is gonna be zero.","width":541},{"text":"We're happy?","width":129},{"text":"If they differ, then it's gonna be minus one.","width":261},{"text":"And this is why I put the half here, so that at the end, we will get just 1.","width":263},{"text":"So we get this step function here.","width":274},{"text":"And these other loss functions are also important.","width":178},{"text":"The square loss is here at the bottom, it is the green curve.","width":266},{"text":"I just shifted it by one.","width":184},{"text":"This is the x-axis here.","width":135},{"text":"In all cases, it's y*f(x), except for this square loss.","width":403},{"text":"And another important loss function, for example, is this exponential loss: the red curve here.","width":535}],
//   [{"text":"First of all in the registration corner.","width":185},{"text":"Uhhh at least five more students uh registered between Tuesday and Thursday as far as I know.","width":533},{"text":"Now the situation now is that you have to get my signature in order to add the class as far as I understand, so people that have forms here, meet me after the the lecture outside and I'll sign all of this and I'll do this after each one of the coming lectures uh.","width":1424},{"text":"My uh my assumption is (and I see too many open seats here) that uh there's gonna be room now in the class uhh for people are on the waiting list once the problem set uh was read by students and they realized they actually have to do some work so uh so I think it should be okay uh.","width":1460},{"text":"It could take some time.","width":184},{"text":"If you are thinking that you're gonna drop the class, do it as soon as possible.","width":384},{"text":"If you have friends that you know are gonna drop the class, please ask them to do it as soon as possible because people are waiting and uh they are anxious about uh this.","width":647},{"text":"Uh.","width":242},{"text":"Homework one is out as we said.","width":153},{"text":"We talked a little bit about it on Tuesday.","width":152},{"text":"Hopefully uh you get all your answers uh all of your questions answered.","width":424},{"text":"Any important questions that we should discuss now?","width":514},{"text":"No questions.","width":64},{"text":"Okay.","width":63},{"text":"Uh.","width":55},{"text":"I wanna remind you that this evening we have another lecture.","width":243},{"text":"Uh.","width":19},{"text":"7 o'clock here.","width":118},{"text":"Hopefully many of you will come.","width":168},{"text":"Uh.","width":187},{"text":"And that's it.","width":180},{"text":"Okay, so what did we do.","width":105},{"text":"I mean we talked about linear classification last time, and we gave an important learning algorithm, so I'm just kind of reminding you what we've done, and toward the end of last time, uh I actually answered some very important questions that people came to ask me after the lecture, and I'm saying this is a reminder that please ask me these questions during class because now only three of you have uhh the answers, and the rest of you don't.","width":2064},{"text":"So one of the important questions was: Are your features independent?","width":355},{"text":"How do you make sure your features are independent?","width":468},{"text":"Any idea?","width":146},{"text":"Any suggestions?","width":186},{"text":"Because we always draw this you know as kind of orthogonal x's.","width":301},{"text":"Uh.","width":151},{"text":"But?","width":322},{"text":"How do you make sure the features are independent?","width":680},{"text":"We don't!","width":48},{"text":"That's the answer.","width":38},{"text":"Alright so if you remember the features that I showed as an example last time, we had I showed a long list of features.","width":548},{"text":"Uh.","width":59},{"text":"Go and look at it again.","width":69},{"text":"One of them looked something like uh the word after the target is the T-H-E and another one was the part of speech after the target is determinable.","width":1122},{"text":"These two features overlap largely when the 'THE' feature is on, the determinable feature is also on.","width":766},{"text":"The other direction is not always true.","width":207},{"text":"But the features are not independent.","width":93},{"text":"You will never be able to make them independent, and none of the theories that we're gonna deal with are in general in this class (definitely the first class of the class) we'll not care about it.","width":949},{"text":"Feature will not be independent.","width":85},{"text":"They'll be partly overlapped.","width":78},{"text":"They'll partly overlap.","width":66},{"text":"Sometimes they'll be identical.","width":121},{"text":"Whether they're syntactically identical or just happen to be identical in the data (they fire together all the time/ they're on together all the time), uh this could happen.","width":830},{"text":"So that's one important question.","width":126},{"text":"The other important question uh I wanna go back to...","width":360},{"text":"something is happening, can you still hear me over there?","width":453},{"text":"Yeah.","width":105},{"text":"Last row?","width":115},{"text":"Okay cuz from here it sounds differently.","width":287},{"text":"Uhh.","width":202}],
//   [{"text":"Okay so, so we ended last time quickly by talking about a few issues, right?","width":462},{"text":"So we talked about stochastic-- we talked about two algorithms: stochastic gradient descent, or online gradient descent, and gradient descent umm, and and I wanted to make a few comments, first of all in terms of um computational issues.","width":1279},{"text":"I quickly ran through this.","width":156},{"text":"It's really an important question, that you want to get some understanding to.","width":408},{"text":"How long would it take you to learn?","width":82},{"text":"How many examples do you need to see, so that you know well, what I've learned from this training data is going to do well in the future.","width":685},{"text":"And we're going to get to it a little bit later in the semester, but its important to realize that, uhh, this is one way to ask the question.","width":481},{"text":"Right?","width":11},{"text":"So you know that what I'm going to learn from my training data is not going to be exact.","width":284},{"text":"We already discussed this and gave a few examples.","width":182},{"text":"So you can ask the question of umm, I just want to ensure that what I've learned, the linear threshold unit has an error rate of less than epsilon, with high probability.","width":1005},{"text":"So with high probability, I'm not going to make a lot of mistakes.","width":390},{"text":"How many examples do I need to see to guarantee this?","width":171},{"text":"And it turns out that it's not that many, its actually linear in the dimension.","width":418},{"text":"As you can see, and inversely proportional to linear in epsilon, and log one over delta.","width":392},{"text":"Which if you think about it, it's not so bad, we're going to talk about it a little bit later.","width":265},{"text":"And we're going to actually prove something like this.","width":170},{"text":"So that's one important question, the second important question is that of computational complexity.","width":327},{"text":"I already told you that if you have just any data, and you want to learn a linear classifier that minimizes the number of mistakes, that is, that minimizes the number of pluses that happen to be classified on the minus side, plus the number of minuses that happen to be classified as pluses, that's NP-hard.","width":1561},{"text":"However, if I tell you ahead of time, this data is linearly separable, it turns out that the algorithm, an algorithm that finds the linear uhh separator, that's a polynomial time algorithm.","width":982},{"text":"And the proof for that in fact you're going to give in your homework one when you show that it's possible to formulate it as a linear program.","width":667},{"text":"Okay?","width":19},{"text":"Is it clear that what you're doing is showing that the algorithm is polynomial?","width":375},{"text":"Because we know that there is a polynomial time algorithm for linear programming.","width":234},{"text":"Now the question that I was asked, so, why do I even bother to learn this stochastic gradient descent or many other online algorithms that we talk about?","width":538},{"text":"There is a closed form solution.","width":131},{"text":"Take all the data, put it in a matrix, and you can write down a solution.","width":617},{"text":"So why do I bother uhh, about algorithms?","width":475},{"text":"Why not just compute the inverse or the pseudo-inverse of this matrix that contains all the data?","width":341},{"text":"[student's answer] Why that's basically the answer.","width":509},{"text":"So I don't want to put all the data that I have in one matrix and invert it, maybe if I run an online algorithm, I won't even have to touch all the data.","width":929},{"text":"Uhh, there are a few other issues uhh, computational issues that have to do with these solutions.","width":462},{"text":"In fact even when you compute this inverse, really what you are doing is an iterative algorithm.","width":403},{"text":"So you are just hiding the iteration, the iterative computation, in another way.","width":384},{"text":"But these are important algorithms.","width":147},{"text":"So, we will also be able to say something about online algorithms in terms of how good are they, in terms of the difficulty of the data, which is something you cannot say, in an easy way at least, uhh if you don't think about it this way.","width":1008},{"text":"What we'll see is that uhh online algorithm have uhh generalization ability, let's say, that is quadratically inverse uhh to the margin.","width":928},{"text":"Which hand-wavingly how difficult the problem is, how far are the points that you want to separate from the best linear classifier that you can learn.","width":693},{"text":"Umm, okay.","width":119},{"text":"So, anyhow, we're going to talk about a lot more algorithms.","width":220},{"text":"Umm.","width":142},{"text":"All of them turns out to be one way or another algorithms for linear classifiers.","width":445},{"text":"There uhh algorithmically they look very differently, they have different guarantees, they work under different conditions, uhh different ways but all of them are linear algorithms which you should be okay with now, given what we said last time in terms of the expressivity.","width":1101},{"text":"Okay so, so I answered two of the three questions that I got last time, and the reason is that I don't remember the third question, uhh but if you still have this question, it's a good time now.","width":918},{"text":"Questions?","width":281}],
//   [{"text":"for three, and represent the data this way.","width":395},{"text":"Alright, so for two, sixteen choose two cominations for three, about 560 numbers.","width":610},{"text":"And of course if n is a hundred, it becomes impossible to deal with.","width":525},{"text":"And this is only for three dimensional tables on a a hundred attributes, and of course if you want more, you can uh...","width":501},{"text":"So the question is that of data representation, at least one of it is data representation.","width":377},{"text":"How do we take data that have a lot off attributes, and represent it in a way that we can understand what's important, what are the the key attributes in this data, what's unimportant.","width":764},{"text":"So you need to figure out a way to represent it, and such a way that the important attributes pop up.","width":501},{"text":"And information theory is gonna be the tool that we are gonna use to do something like this.","width":379},{"text":"It's gonna be able to tell us something about how to representing in a way by surfacing up somehow the important attributes.","width":564},{"text":"An the decision tree is gonna be the tool to do it.","width":208},{"text":"So what are decision trees.","width":143},{"text":"It's a data structure, it's a hierarchical data structure that represents data by divide and conquer strategy.","width":499},{"text":"You can think about it from the learning perspective as an non-parametric classification method.","width":366},{"text":"It can also be a regression method.","width":132},{"text":"So we are only gonna talk about the case where you have categorical labels, but you can also think about decision trees essentially the same algorithm that produces read values, uh, numbers.","width":820},{"text":"And the idea is given a correction of an example we wanna be able to induce a decision tree that represents the data.","width":492},{"text":"For example here's the collection of shapes that I'll divide into three categories.","width":503},{"text":"And they have multiple attributes, the color, and the shape for example.","width":442},{"text":"True?","width":43},{"text":"And I could say if the color is red, the type is B.","width":473},{"text":"Simple decision.","width":75},{"text":"If the color is green, then I don't know, I need to look at something else, I need to look at the shape.","width":458},{"text":"So I'm gonna get something like this.","width":135},{"text":"So if the color is red, I know exactly it's type B.","width":218},{"text":"If the color is green, I still don't know, I have to look at the shapes.","width":183},{"text":"Circle is A, square is B.","width":183},{"text":"So that's the type of things.","width":124},{"text":"So what I have here is an decision tree of the two features, two attributes, color and shape.","width":578},{"text":"And then I have the label, which is one of A, B, or C.","width":260},{"text":"Nodes are tests for the feature values.","width":292},{"text":"And the leaves are the categories, the labels.","width":146},{"text":"So once I have this decision tree, evaluating it is very simple.","width":529},{"text":"I'm gonna give you a shape, and you will be able to quickly go down the tree and tell me whether it's type A, B, or C.","width":585},{"text":"Agreed?","width":149},{"text":"The difficulty is gonna be how to learn this decision tree.","width":299},{"text":"I'm gonna give you data, how to learn a good decision tree for this data?","width":343},{"text":"So again, think about this data here.","width":187},{"text":"And we induce somehow the decision tree.","width":202},{"text":"Seems somehow reasonable, at least correct.","width":157},{"text":"I don't know if who can do a better one.","width":143},{"text":"In fact, we haven't even talk about what does it mean to be a better decision tree.","width":446},{"text":"So before we get to learning, I wanna talk about expressivity.","width":266},{"text":"To simplify the discussion on expressivity, I changed the categories here to only be plus and minus.","width":379},{"text":"So it's a binary label, boolean function.","width":315},{"text":"So decision trees with pluses and minuses, the leaves are boolean functions.","width":451},{"text":"We talked about expressivity of boolean functions, so e talked about linear classifiers and said not all boolean functions can be written as linear functions.","width":737},{"text":"Not all boolean functions can be written as disjunctions.","width":236},{"text":"This is your homework.","width":75},{"text":"Some I'm at least understanding it as your homework assignment.","width":315},{"text":"What about decision trees?","width":81},{"text":"what can you say about the expressivity of decision trees.","width":354},{"text":"What kind of boolean functions can be expressed or cannot be expressed?","width":714},{"text":"So you have a bunch of features or attributes, or in this case, color and shape, and you wanna write a boolean function, something that maps.","width":915},{"text":"[Student mumbling] So each path is a conjunction, so the color is blue and the blue and the shape is a triangle that's negative example.","width":1004},{"text":"Or the color is green and the shape is square, positive example.","width":423},{"text":"So a path is a conjunction.","width":175},{"text":"But overall the decision tree [Student mumbling] .", "width":652}],
//   [{"text":"Yes?","width":36},{"text":"It can represent any boolean functions, because it is a combination of conjunctions, disjunctions, and negations.","width":331},{"text":"Right.","width":37},{"text":"So, so it turns out if each path is a conjunction, you can actually think about a decision tree as a disjunction of conjunctions, or something that is called DNF.","width":707},{"text":"Disjunctive Normal Form.","width":76},{"text":"So, hopefully, everyone has seen this.","width":134},{"text":"If not, google 'DNF representation' and figure out what it is.","width":397},{"text":"So, I can always write...","width":182},{"text":"So, first of all.","width":126},{"text":"DNF- disjunctive normal form, which is this AND this AND this AND this OR this AND this AND this AND this OR this AND this AND this.","width":561},{"text":"Where this could be variables or the negation of the variables.","width":290},{"text":"This is the Disjunctive Normal Form.","width":132},{"text":"And it's a universal representation.","width":133},{"text":"Any boolean function can be represented this way.","width":186},{"text":"In the decision tree, it is easy for you to see that it can express any DNF.","width":342},{"text":"Because as we said every path is a conjunction.","width":289},{"text":"So, it's a universal representation.","width":204},{"text":"The hypothesis space then is...","width":238},{"text":"all boolean functions.","width":139},{"text":"If you remember when we started with this table when I gave you a few examples, and you said: 'well, it could be this function, it could be this function, this function...' And we said: 'well, in order to really learn, we're gonna restrict the hypothesis space.' Because otherwise- too many options.","width":1246},{"text":"Who knows which one is better?","width":140},{"text":"In decision trees the space is the universal representation.","width":405},{"text":"So every boolean function can be expressed this way.","width":268},{"text":"For example, in the case of that we have here.","width":214},{"text":"What do we have?","width":52},{"text":"We have two features: color and shape.","width":260},{"text":"Each one of them has three potential values: blue, red, green.","width":377},{"text":"And the shape could be circle, square, triangle.","width":221},{"text":"This is the size of the hypothesis space, just as an exercise for you to realize, right?","width":498},{"text":"So I have two dimensions, three values each.","width":164},{"text":"So the instance space is of size nine.","width":216},{"text":"Right?","width":111},{"text":"Three times three.","width":109},{"text":"And therefore, the number of functions, boolean functions on nine variables will be two to the nine.","width":665},{"text":"Large number of decision trees.","width":151},{"text":"Okay, so is that good or bad?","width":160},{"text":"The fact that we can express everything?","width":486},{"text":"Let's take a vote.","width":59},{"text":"Good, who thinks it's good?","width":220},{"text":"Okay, I have a few votes for good.","width":166},{"text":"Who thinks it's bad?","width":182},{"text":"Okay, it's about a tie.","width":99},{"text":"And most of you don't think.","width":197},{"text":"So, why is it good?...","width":509},{"text":"Why do you want to have an expressive space?","width":1142},{"text":"So we can fit the data well if we have an expressive space.","width":273},{"text":"Because any set of examples that you will give me, there is a decision tree that is gonna fit this data.","width":459},{"text":"Right?","width":21},{"text":"That's the good thing about expressivity.","width":193},{"text":"What's bad about it?","width":323},{"text":"I can fit everything.","width":97},{"text":"So, that's exactly the same.","width":109},{"text":"So, it's good that I can fit everything, but on the other hand I can fit noise.","width":319},{"text":"And maybe what I'm learning is some coincidence that occurs in the data, and not necessarily something that will generalize well.","width":529},{"text":"So this is the risk, and we're gonna go back to this important point later today.","width":415},{"text":"So, but now, let's start moving toward learning it.","width":464},{"text":"So, there are several algorithms that can learn this.","width":338},{"text":"Actually, pretty good and efficient algorithms for processing large amounts of data.","width":519},{"text":"Most of the algorithms don't deal well with huge feature spaces like other algorithms that we'll see in this class.","width":456},{"text":"But they are relatively good.","width":119},{"text":"There are a lot of methods for dealing with noisy data and a few other issues that we're gonna talk about today.","width":494},{"text":"So, how do we think about boolean decision trees of decision boundaries?","width":481},{"text":"In this case, what I put here is in the....","width":288},{"text":"as features- real valued functions.","width":210},{"text":"And the conditions are essentially x is less than y less than or greater than.","width":313},{"text":"So you can really think about it in the plane as defining axis-parallel rectangles in the plane.","width":581},{"text":"This is one way to think about it.","width":122},{"text":"Okay.","width":31},{"text":"So, we said that we can represent any boolean functions.","width":253},{"text":"It's a very natural representation.","width":125},{"text":"You can think about it as a kind of a twenty question game.","width":301},{"text":"Is it this?","width":71},{"text":"What is the color, what is the shape, and so on.","width":179},{"text":"And then, it's easy to do.","width":124},{"text":"So this is the type of decision tree that we're gonna have.","width":230},{"text":"So, as we said, given the dataset, there are gonna be many many decision trees.","width":433},{"text":"And the question is which one we wanna learn.","width":274},{"text":"Okay.","width":40},{"text":"So, I actually wanna go back to one slide that I skipped at the beginning because I think now that...","width":837}],
//   [{"text":"I wanna say something about it.","width":273},{"text":"Where is it...","width":617},{"text":"OK.","width":34},{"text":"Here.","width":66},{"text":"So, before we get to algorithm, uh..., the question why do we want to learn decision trees.","width":642},{"text":"So as I say here, the study of decision trees can share a lot of on issues of what are good features, and it can also be used as a way to learn important, good features.","width":1114},{"text":"but i also want to address this issue which I kind of skip before.","width":507},{"text":"So one answer that i typically used when people ask me why do we need to learn decision trees is actually the answer to this.","width":720},{"text":"What's the best learning algorithm?","width":146},{"text":"I am sure you know the answer to this.","width":250},{"text":"It's in the newspaper now.","width":222},{"text":"But typically what's in the newspaper is a trial.","width":278},{"text":"So there is no good answer for what's the best learning algorithm, because that depends on what data you have.","width":514},{"text":"when do you want to learn.","width":84},{"text":"how do you want to learn.","width":82},{"text":"what is the domain.","width":161},{"text":"Lots of issue.","width":138},{"text":"But if you know very little about your domain, about your data, and you still want to choose a learning algorithm, probably the best learning algorithm to use is gonna be either based on decision tress, it's gonna be learn a lot of small decision trees.","width":1649},{"text":"and then use learn function on top on this small decision tree.","width":322},{"text":"so learn a linear classifier on small decision trees.","width":353},{"text":"this is by the way gonna be the content of homework 2.","width":202},{"text":"you are gonna do this.","width":174},{"text":"and you can learn this linear function over small decision tress multiple ways.","width":297},{"text":"either just do an algorithm that we talked about last week or you can do something that is called boosting and you'll have boosted decision trees.","width":585},{"text":"we are gonna see this in a couple of week.","width":170},{"text":"This is a very good generic learning algorithm and this is gonna do well almost in any context.","width":584},{"text":"and provide that another good reason for why we wanna learn decision trees.","width":365},{"text":"Okay.","width":22},{"text":"with this, let's start learning.","width":221},{"text":"the example we will gonna start playing with is gonna be this example.","width":372},{"text":"Will i play tennis today?","width":166},{"text":"The answer is no, because we have a meeting at 7.","width":217},{"text":"So don't forget that.","width":230},{"text":"But that's not gonna be one of the features.","width":186},{"text":"so the feature that I am gonna use are these four features.","width":294},{"text":"outlook, temperature, humidity and wind.","width":168},{"text":"each one of them has multiple values.","width":194},{"text":"the label are gonna be just yes or no.","width":243},{"text":"with this, we wanna be able to collect the data and learn the function.","width":466},{"text":"so again this is the way i define my instance space.","width":476},{"text":"four feature.","width":83},{"text":"each one is 2 to 3 values.","width":119},{"text":"here is data.","width":210},{"text":"ok.","width":18},{"text":"so now the question is how do i learn it.","width":186},{"text":"how do i learn a decision tree presentation using this.","width":403},{"text":"any suggestions?","width":50},{"text":"what do i have to start doing?","width":582}],
//   [{"text":"you want a decision tree, you know how a decision tree looks right?","width":266},{"text":"so what do I have to do?","width":203},{"text":"I have to choose one attribute that gonna be the root of the tree.","width":329},{"text":"So we have to figure out what's the good way to choose an attribute.","width":262},{"text":"and once I have an attribute, that I decide that's someone who is the best.","width":290},{"text":"what do i do?","width":242},{"text":"I continue basically, right?","width":89},{"text":"So I recursively choose the best the second attribute and so on.","width":257},{"text":"so that's what we gonna do, right?","width":126},{"text":"we gonna take this data as in Batch, all the data is valuable to us, and use this as a way to recursively build a decision tree top down.","width":903},{"text":"that's all we gonna to do.","width":64},{"text":"so we choose one feature, so we have to split it.","width":338},{"text":"and we learn another decision tree, and another here, another here and that's the algorithm so let's assume that S is the set of examples.","width":708},{"text":"Label is the target attribute.","width":166},{"text":"yes or no in this case.","width":101},{"text":"attributes are the set of measured attributes that we have, and this is the algorithm if all of that examples are labeled the same, then it's easy, return a single node.","width":677},{"text":"otherwise, let A be the set of attributes, that classify S let A be the attribute in Attributes that is best in classifying S, so basically this is the root node, this is the first decision I have to make, we don't know how to make it yet, but let's assume we can find this, now we are gonna split on A, for each possible value v of A, I am going to add a new tree, and a new branch corresponding to A equal to v.","width":2171},{"text":"I am going to look at several subsets, Sv is going to be the subset of examples in S, for which the attribute A is value v.","width":741},{"text":"If I have no one, no example with this value, I just add a leaf node with the most common value in S.","width":760},{"text":"Otherwise, I am gonna take the sets and learn recursively a new decision tree for all these sets with all attributes except for A because I already used A.","width":1061},{"text":"Now one question here is why do I need this here?","width":227},{"text":"If Sv is empty, who cares?","width":243},{"text":"Why do I need to add the leaf node, and I am suggesting here to do it with the common value that labeled in S.","width":578},{"text":"here?","width":714},{"text":"Right, so it could be in the future, I am gonna get an example for which this is the value of A.","width":549},{"text":"I have to be able to say something about it.","width":174},{"text":"So even if my training data doesn't have it, I need to do it.","width":258},{"text":"Okay, so now the [gibbish] of the skeleton for an algorithm.","width":213},{"text":"we all agree that picking the root attribute is the important choice, and before that how do we decide what's good and what's bad.","width":792},{"text":"Right?","width":15},{"text":"I can just pick randomly an attribute.","width":176},{"text":"You would agree this is probably wrong.","width":186},{"text":"So we need some guide and see how would we go about, what kind of trees we need.","width":901},{"text":"Right, so that's actually the idea.","width":132},{"text":"We want the smallest decision tree, why do we want the smallest decision tree?","width":320},{"text":"So this is the an outcome rhythm principle that we gonna .", "width":246}],
//   [{"text":"talk a lot about during the class and in fact make it rigorous in a couple of weeks.","width":400},{"text":"At this point we just have the intuition based on the discussion forming before.","width":261},{"text":"The small is good because small is likely to generalize better.","width":343},{"text":"Right?","width":38},{"text":"If you manage to fit your data with a small tree, it is probably gonna do better because with a large tree, you can fit any data.","width":573},{"text":"you can always do it with a large tree.","width":158},{"text":"However, we can not do it.","width":97},{"text":"So just like we said last week, a lot of this problem are computationally hard.","width":456},{"text":"Given data, finding the minimum decision tree that is consistent with the data, there always is one because decision tree is hard expressive, but finding the minimum one is NP-hard.","width":880},{"text":"So what we gonna to do is we gonna forget finding the minimum one and come up with heuristics, a greedy heuristics that search for a simple tree not necessarily the optimal tree.","width":1043},{"text":"Ok, so with this at hand, let's think about picking the root attribute.","width":278},{"text":"and let's start with this toy example here.","width":166},{"text":"I am gonna give you this data, it's 200 examples.","width":342},{"text":"two attributes A and B, when A and B are zero, it's a negative example.","width":367},{"text":"There are 50 examples like this.","width":161},{"text":"Zero and one, negative example also 50 examples.","width":220},{"text":"One and zero, negative example but my training data is no example.","width":296},{"text":"And one and one, positive example a hundred.","width":186},{"text":"a hundred examples like this.","width":202},{"text":"So, which attribute would you choose for the root of your tree.","width":359},{"text":"It's gonna be a simple tree but nevertheless you can choose whether A or B is your preferred root attribute.","width":571},{"text":"What would you choose?","width":365},{"text":"yeah.","width":840},{"text":"Ok, before getting to sophisticated concept just by looking at the data, can you give me a simple argument why you like A or B?","width":856},{"text":"No no, I think you argument is exactly right.","width":744},{"text":"but for this specific data, you can say in a much simpler way.","width":1207},{"text":"Ok, so you choose B because if you choose B and split on it.","width":260},{"text":"You get an even split of the data.","width":259},{"text":"Ok, that's one argument.","width":154},{"text":"Over the..","width":186},{"text":"why?","width":310},{"text":"Ok, another way of saying this.","width":185},{"text":"I also vote for A.","width":163},{"text":"why?","width":409},{"text":"So that's an argument similar to what you said which is if I split on A, I immediately know what's happening for almost all of the data.","width":632},{"text":"or of a lot of the data.","width":131},{"text":"So if I choose A, I get purely label nodes right?","width":312},{"text":"So I know if A is zero, it's a negative example.","width":245},{"text":"if A is one, it's a positive example.","width":177},{"text":"Done.","width":116},{"text":"On the other hand, if I choose B.","width":269},{"text":"I have to ask another question.","width":199},{"text":"if I choose B, if it's zero, it's ok I know.","width":250},{"text":"But if it's one, I have to ask another question.","width":168},{"text":"So I am much closer to the leaves if I am choosing A.","width":443},{"text":"Ok?","width":19},{"text":"So that's kind of the way that I expressed before, I want a smaller tree.","width":363},{"text":"If I choose A, it's more likely that I get a smaller tree.","width":439},{"text":"Ok?","width":22},{"text":"So, now let's assume that I change the data a little bit.","width":377},{"text":"I am adding three examples of the tie A equal to 1, B equal to 0.","width":454},{"text":"So this is my data now, now the argument is not gonna be that simple, because if I split on A, I still don't know.","width":822},{"text":"What would you choose now?","width":580}],
//   [{"text":"It's only positive examples, and for outlook it's again a split of three uh three- two, so I know what it is.","width":455},{"text":"So now I have to compute the average of this, and this is the expected entropy: 5* 14 that come from the outlook, 4 the count from zero, and 5/14 that come from uh from uh from rainy and that's what I get.","width":1145},{"text":"Overall this is my information gain.","width":149},{"text":"I have to do this for each one of the features.","width":157},{"text":"Let's take one more example for humidity, and again if we look at humidity, you can see uh it can take the value of high where the split is 3/7.","width":771},{"text":"There are seven examples[?].","width":188},{"text":"If you take normal, the split is better: 6- 1.","width":386},{"text":"Uh and this is the expected entropy.","width":186},{"text":"Uh and the information gain is smaller.","width":211},{"text":"And you can do uh the same thing for all four.","width":255},{"text":"What you will get is the outlook is your best choice.","width":262},{"text":"So I'm gonna start with splitting on outlook.","width":239},{"text":"And I finish the first step of the algorithm.","width":133},{"text":"Now I'm going to repeatedly do the same thing.","width":152},{"text":"First of all I have to narrow down my dataset.","width":216},{"text":"So, now I have three datasets.","width":193},{"text":"Outlook equal sunny has five examples.","width":229},{"text":"And I'm going to start computing learning from there.","width":319},{"text":"Overcast has four examples, but I don't need to do anything because I know the label.","width":354},{"text":"And rain has five examples and I have to continue from there.","width":336},{"text":"So let's uh just continue one more step.","width":267},{"text":"Ah we're going to continue until every attribute is included in the path or uh all the example are have the same labels, so there's nothing to do.","width":757},{"text":"Specifically here, I'm gonna choose for example the sunny case here, and sunny has three values so I'm going to look at humidity, temperature, wind.","width":654},{"text":"For example in the case of humidity, this is my data set.","width":255},{"text":"Uhm.","width":75},{"text":"You can see that high is no, normal is yes....","width":630},{"text":"So I have 0 on three examples and 0 on two examples.","width":352},{"text":"I started with .97 so my information gain is .97.","width":350},{"text":"And you can do the same thing for all the others.","width":165},{"text":"Uh, and you see get uh this decision tree.","width":434},{"text":"Okay, just to summarize, we ask the question first: do we have a uniquely defined class?","width":478},{"text":"If yes, then no need to do any work.","width":294},{"text":"Otherwise we find the feature with the most information gain.","width":236},{"text":"Basically we go over all attributes, x_i's, and i is the argmax of the information gain of X_i relative to the set S.","width":764},{"text":"We add children to S for all values uh and we continue down this to induce the decision tree for this split of the data.","width":847},{"text":"That's what we get.","width":40},{"text":"Questions?","width":173},{"text":"[student] Yeah.","width":522},{"text":"So let's see what is the time complexity of this algorithm.","width":375},{"text":"In terms of computation, how much work did we do?","width":714},{"text":"But information gain computation is uh is a simple function of how many values each attribute has time the number of attributes.","width":773},{"text":"Right?","width":94},{"text":"And the amount of data that we have, so it's going to be linear in the data that we have, the number of uh attributes, and the number of values that each attribute can take.","width":823},{"text":"Very simple computation right?","width":100},{"text":"So the learning algorithm in this sense is quite efficient even without doing anything sophisticated, you can do, you know, some more interesting data representation (data structure that will make it easier), but essentially this is rather easy to do.","width":1178},{"text":"Alright, whether the best tree is another question.","width":234},{"text":"We just run this algorithm and believe in it, but but computationally is it not that difficult.","width":386},{"text":"What have done?","width":71},{"text":"We actually did search, but we did search without looking back right?","width":332},{"text":"We did greedy search uh.","width":172},{"text":"We didn't find the decision tree because we realize that finding the best one is NP-Hard, but we have some greedy heuristic that, as I said, does hill-climbing without any backtracking.","width":857},{"text":"Uh, now it feels like we make statistically based decisions based on all the data right?","width":439},{"text":"I give you all the data, and you use this to compute the decision tree.","width":407},{"text":"Is that really correct?","width":560}],
//   [{"text":"It's a birch algorithm, that's true.","width":316},{"text":"But we made a lot of decisions right, we the decision on the rule.","width":382},{"text":"We've all the data.","width":223},{"text":"The next decision was on what?","width":552},{"text":"After it shows the rule, it split the data and...","width":375},{"text":"choose another rule.","width":149},{"text":"So as you go down the tree, what's happening?","width":380},{"text":"In terms of data?","width":285},{"text":"Yes.","width":154},{"text":"Using less and less data as we go down, so you could say that the decision you are making towards the bottom of the tree are less robust statistically actually using a lot less data in making that.","width":1107},{"text":"And that's an important thing to remember because that has something to say about generalization eventually and what do we want to fix to get better generalization?","width":721},{"text":"Ok.","width":17},{"text":"So we talked about learning decision trees and I want to talk a little bit about overfitting now.","width":444},{"text":"Talk about extensions probably later in the evening and principle of experimental machine learning.","width":518},{"text":"The key question we want to address is when we want to learn something, how do we avoid overfitting?","width":532},{"text":"Before I want to say a few things about the history of decision trees.","width":258},{"text":"Because it's important to realize, first of all its been around for may years.","width":431},{"text":"It's actually not invented by computer scientists, like all good algorithms.","width":361},{"text":"It was invented by psychologists as a way to represent data more compactly.","width":295},{"text":"In the 60's, of the previous century.","width":359},{"text":"And In the 70's Quinlan developed ID3.","width":440},{"text":"Invented the information gain heuristic we just talked about, I think in the 80's there is a famous book called CART, for classification regression trees.","width":813},{"text":"It's about the same time, late 70's, early 80's.","width":340},{"text":"That ID3 became popular, forced decisions, two of them are famous- Reimann and Freedman.","width":523},{"text":"Wrote a book called CARTS, that basically studied decision trees and closed a lot of the problems that people had with it.","width":601},{"text":"This still was maybe eased a little bit active research areas because it is a popular algorithm.","width":539},{"text":"But for the most part, there are good implementations.","width":221},{"text":"Most of the implementations today are based off of Quinlan's algorithm that is called C4.5.","width":391},{"text":"Now there is a version C5 and a few others, but these are commercial.","width":280},{"text":"And one of the key ways people are using decision trees today is not by learning one big decision tree, the way we did earlier, but rather using it on the data multiple times, to learn a lot of small decision trees, and then combine them in some ways.","width":1388},{"text":"And this turns out to be, a very very good general purpose learning algorithm.","width":289},{"text":"As I said, you are going to do this on your next problem set.","width":273},{"text":"And we are going to talk a lot about how to combine these decision trees.","width":356},{"text":"Ok now let's just move now from the basic algorithm to talk about some issues that come up.","width":428},{"text":"The first one, here is another example.","width":176},{"text":"SO I already learn't the tree.","width":206},{"text":"Here comes another example.","width":167},{"text":"Let's look at this example.","width":189},{"text":"What can you say about it?","width":635}],
//   [{"text":"Right so you that to achieve some performance on the [inaudible] yet something else?","width":2495},{"text":"[Student Answer] Ok but what you are explaining is the beginning of the orange curve, which is right.","width":901},{"text":"So the beginning of the orange curve, we can say if the tree is not large enough it cannot explain the data and it cannot explain any data; not the training, not the test.","width":933},{"text":"Until it gets to some size and we get some performance on the test data.","width":420},{"text":"At some point, this is the important issue that you have brought up.","width":314},{"text":"At some point it fits exactly or close to be fitting exactly the training the data.","width":587},{"text":"And then potentially deviates from, does not explain other data points that you have not seen before.","width":576},{"text":"So this is an important intuition to get.","width":183},{"text":"Why is that the case.","width":71},{"text":"Why would some point, even though the performance of the training goes up, the performance on the test would go down.","width":796},{"text":"Yeah?","width":142},{"text":"[Student Answer] Meaningless things that are important on the training data actually dictate the tree.","width":1043},{"text":"So one good way to thing about it, go back to the example that we gave at the beginning of the semester, the badge data.","width":582},{"text":"So i gave you a bunch of examples, 200 examples, you can give this to your algorithm and potentially, if you use the right features, learn the tree that is identical to the list.","width":1028},{"text":"What does it mean identical to the list?","width":179},{"text":"It says yes on every name that is positive in the list and it says no on everything else.","width":533},{"text":"You can imagine learning something like this right?","width":178},{"text":"its very easy to do.","width":61},{"text":"Think about a A DNF right, so we talk about the fact that a decision tree is a DNF.","width":544},{"text":"Take each one of your positive example.","width":128},{"text":"A positive example is a list of a bit vector right.","width":446},{"text":"You can generate a conjunction that is identical to this example by taking all those that are on to be your variables to your conjunction and leaving anything else out.","width":798},{"text":"So this conjunction would be exactly this example.","width":237},{"text":"Now take all the positive examples and do one conjunction for each one, and do a disjunction between them.","width":648},{"text":"Are you with me?","width":126},{"text":"That hypothesis that I just generated is going to say yes on all positive examples and only on those positive examples, nothing else.","width":789},{"text":"Now this is a good hypothesis because it is consistent with the data.","width":322},{"text":"But clearly it would not be good because any other hypothesis that has other positive examples would disagree with you.","width":700},{"text":"You will not do well on all of the positive examples outside those that you have seen in training.","width":451},{"text":"So this is an example that shows if you are too good on your training data, you are likely to suffer on your test data.","width":653},{"text":"Okay?","width":45},{"text":"Is that point clear?","width":65},{"text":"THis really is an important point that hopefully you develop an intuition once you start actually playing with it.","width":525},{"text":"Okay.","width":17},{"text":"So that is important, so we want to talk about this in a slightly more rigorous, semi-rigorous way.","width":562},{"text":"And we are going to make some definition, so the empirical error is going to be the percentage of items in the data set, the training set, that is misclassified by the classifier.","width":702},{"text":"So here I am putting another curve and I am claiming here that the empirical error looks like this.","width":829},{"text":"Agreed?","width":170},{"text":"Why?","width":245},{"text":"Yeah?","width":13},{"text":"[Student Answer] Right.","width":373},{"text":"But how do you explain the behavior of this as the function of the model complexity?","width":665},{"text":"So the error .", "width":662}],
//   [{"text":"But this is training data.","width":241},{"text":"Right?","width":17},{"text":"the empirical error its on the training data.","width":359},{"text":"So is that what you expect.","width":74},{"text":"What did you tell me before when we looked at it.","width":387},{"text":"The accuracy goes up and then goes down.","width":575},{"text":"So you agree with this, that the empirical error is going to go down and then go up.","width":482},{"text":"Yeah?","width":38},{"text":"[student question] This is on the training data.","width":596},{"text":"Vote here.","width":72},{"text":"What do you think?","width":260},{"text":"So what should happen to the empirical, to the error on the training data.","width":525},{"text":"Only go down right?","width":195},{"text":"if we are talking training.","width":168},{"text":"Agreed?","width":24},{"text":"So this is just put you to confuse the enemy, to make sure that you are following me.","width":376},{"text":"It has to go down right?","width":148},{"text":"because otherwise what we said in the previous slide was wrong.","width":294},{"text":"So you agree?","width":65},{"text":"On the training data the error should go down until it gets to zero if you have an expressive enough class of learners.","width":706},{"text":"SO now we have, so this is the role of this question marks.","width":370},{"text":"So again what is model complexity for us?","width":369},{"text":"model complexity informally is how many parameters we have to learn.","width":319},{"text":"We are going to make this a little bit more clear as we go for different models.","width":266},{"text":"But for decision trees it is kind of easy, we can think about the complexity as the number of nodes.","width":429},{"text":"We will see that it is not so easy to think about complexity for other function classes, even not for linear separators because what is the difference between this linear separator and that linear separator in terms of complexity.","width":962},{"text":"We have to develop some more theory here.","width":160},{"text":"And then we talked about the expected error which is in the future on the test data, things that we haven't seen during training.","width":635},{"text":"What percentage of items drawn from P(x,y) do we expect to be misclassified by f?","width":397},{"text":"This is what we really cared about right?","width":190},{"text":"this is an indication of how well we generalize.","width":358},{"text":"OK.","width":22},{"text":"So how can we explain this.","width":156},{"text":"So we are going to define two notions, one is them is called the variance of the learner and I'm going to argue that the variance of the learner is going to go up as the complexity of the model, the number of parameters, the size of the trees goes up.","width":1096},{"text":"So informally you can think about htis as how perceptive is the learner to minor changes in the training data.","width":419},{"text":"So as the model complexity goes up, the learner is more susceptible to minor changes to training data because it can express it.","width":794},{"text":"You change it a little bit, if you have a very big tree, it would be a different tree.","width":375},{"text":"Right?","width":17},{"text":"If you have a small tree, it couldn't care less.","width":180},{"text":"It doesn't know how to represent slight changes in the training data.","width":269},{"text":"So the variance increases with model complexity.","width":250},{"text":"Really think about extreme cases, if you have a hypothesis space with one function, you can change the data as much as you want, that is the function we are going to choose.","width":678},{"text":"If you have a lot of function, you can change the data and then the function that you are going to choose from this hypothesis space is going to change.","width":574},{"text":"And therefore the larger the hypothesis space, the more flexible the selection of the chosen hypothesis is.","width":537},{"text":"And a little bit more accurately, you can think about it in the following way, for each data set, think about learning a different hypothesis h(D).","width":698},{"text":"And this will have a different true error e(h) here.","width":578},{"text":"We are really looking for the variance of this random variable.","width":220},{"text":"The random variable which is e(h).","width":231},{"text":"And what i'm claiming is that the variance of this random variable grows as the size of the hypothesis space, or the model complexity, same thing goes up.","width":759},{"text":"That's one component, the second component is the bias.","width":273},{"text":"What do I mean by bias.","width":124},{"text":"And I am going to claim that the bias goes down as the model complexity goes up.","width":363},{"text":"So here the question is how likely is the learner to identify the target hypothesis.","width":376},{"text":"And you would agree with me that if you have a more expressive model, it is more likely to find the right one.","width":421},{"text":"Or to be close to the right target hypothesis.","width":197},{"text":"So the bias is going to be low when the model is very expressive, again we can think about the same way that we thought about before the variance.","width":762},{"text":"The larger the hypothesis space is, the easier it is to be close to the true hypothesis or the more likely it to actually hit the true hypothesis.","width":703},{"text":"Right?","width":43},{"text":"So again we can do it slightly more accurately, we can think about a lot of data sets.","width":393},{"text":"For each data set you learn a different hypothesis h(D).","width":330}],
//   [{"text":"This hypothesis has two errors.","width":224},{"text":"E of H and we're looking at the difference of the mean of this random variable for the true error.","width":454},{"text":"Could be zero but doesn't have to be zero.","width":175},{"text":"So, what I'm claiming is that this difference is going to go down as the size of the hypothesis space goes up, because you're more likely to be very very close to the target hypothesis.","width":937},{"text":"OK, so these two are kind of quantities that can help us to think about our generalization overfitting, and the reason is that if you think about the expected error as the function of model complexity, really the expected error is the sum of the bias plus the variance.","width":1787},{"text":"So the bias says how close are you going to be to the two error, and the variance is going to say what's the variance of this random variable.","width":791},{"text":"And that means because of this behavior that if you have simple models, high bias low variance.","width":628},{"text":"If you have very complex models, the bias is going to be low but the variance is going to be high and therefore the truth is somewhere in the middle.","width":649},{"text":"And this middle is where we're going to start overfitting.","width":296},{"text":"We're going to move from a model that isn't good enough, improving improving improving, and now it's too good on the training and beginning to be not good enough on the test data.","width":836},{"text":"So that's the intuition on why we care about model complexity, so these are the two region that, underfitting region and the overfitting region.","width":756},{"text":"OK, questions?","width":409},{"text":"OK, so we're going to go back to this point a little bit later.","width":298},{"text":"Everything that I said so far can be made more accurate if we fix a loss function, how we measure our performance, and we'll drive the learning algorithm and we're going to develop in a couple weeks a more precise and general theory that trades expressivity of the models, size of the hypothesis space, with the empirical error, and see exactly what the relations are.","width":1916},{"text":"But this intuition is actually going to stay with us.","width":242},{"text":"So, with that we're done with this detour and we can move back to decision trees and think about these ideas in the context of decision trees.","width":893},{"text":"So, we've already talked about pruning, and I just want to mention briefly again two other aspects of pruning.So, you can prune the decision trees as they are Look at the tree and prune the tree.","width":1061},{"text":"A lot of algorithms, in fact, inside the commercial packages you will see conversions of the decision tree to a set of rules.","width":598},{"text":"Sometimes it's more comprehensible to look at a set of rules rather than look at a tree, and you can easily see that you can take the decision tree and convert them to rules.","width":668},{"text":"In this specific case you will say, you look at each path and convert each path into a rule, a conjunction of things.","width":680},{"text":"Now if you decide to do your pruning, so you can do your pruning at the level of the tree, you can also just look at each rules and do the pruning there.","width":638},{"text":"So you look at the rule and consider each component of the rule, each attribute of the rule, attribute equal value in the rule and decide do i want to leave it in the rule or should I drop it.","width":885},{"text":"How would you decide?","width":76},{"text":"You do it in the same way, you leave a validation set and you ask yourself: if i change this rule by dropping something, what's going to be the impact on the performance in the validation set The important thing to realize is that if you drop something from a rule, you generalize it, right?","width":1469},{"text":"it fires more often, right?","width":178},{"text":"If you say this and this and this means that the level is yes versus only this and that, two conditions instead of three, it's a more general rule.","width":845},{"text":"It's going to fire more often It's important to realize that pruning this way and pruning this way are not the same thing.","width":684},{"text":"Why?","width":343},{"text":"Yes?","width":148},{"text":"*Student Answering* We can, but we don't have to.","width":615},{"text":"We can also just say eliminate this trunk from here.","width":591},{"text":"But eliminating this trunk is equivalent to eliminating this.","width":460},{"text":"But what can i do with the rule that I cannot do with the tree?","width":787}],
//   [{"text":"Trees we prune bottom up, right?","width":206},{"text":"We prune from the leaves.","width":184},{"text":"I could decide that I'm pruning this part from the rule.","width":350},{"text":"Once I've written it as rules, everything is possible.","width":239},{"text":"Right, so...","width":92},{"text":"so I don't necessarily prune nodes that correspond to leaves; I could prune things that correspond to some middle node in the tree.","width":690},{"text":"It could happen, right?","width":57},{"text":"So it's really basically what my validation [set] says...[noise].","width":305},{"text":"So, it could be a different thing.","width":120},{"text":"Okay, so let's move to a few other extensions of decision trees and the first thing I wanna talk about is continuous attributes.","width":465},{"text":"So, we've always behaved, before, as if everything was discreet.","width":296},{"text":"Outlook can take three values, rank can take [mumble] you know, each one of these could take...","width":509},{"text":"humidity can take a few values and so on.","width":181},{"text":"It could be that what you have is continuous values.","width":213},{"text":"What do we do?","width":73},{"text":"So the standard way to think about it is: let's discretize it, and the question is only how to discretize.","width":673},{"text":"You can just discretize yourself, you just understand the problem.","width":246},{"text":"If you have something that represents size, let's discretize it with three values: big, medium, small, alright?","width":538},{"text":"Or, you can develope some splitting nodes that are based on threshold: whether the value is less than C, or less than T, or greater than C, and [stammer] now you have...","width":1042},{"text":"you have a boolean, uh, node, attribute: whether A is less than C or not, and you can use the decision tree algorithm as before, right?","width":720},{"text":"Now, it becomes a boolean node: A less than C; A not less than C.","width":413},{"text":"And you can compute information gain, you can do everything as before.","width":318},{"text":"The question, of course is going to be: how do I split in the right way?","width":262},{"text":"How do I choose these [values of] C?","width":153},{"text":"So, one simple way to do it is to say: I'm going to use the label data as a guide in doing it, and I'm going to split in ways that seem to be important, given this specific dataset that I have.","width":826},{"text":"For example, I'm going to take each continuous feature, sort the examples according to the values, and for each ordered pair, (x,y), with different labels, I'm going to choose the midpoint.","width":822},{"text":"If (x,y) has the same label, I'm not going to split them.","width":282},{"text":"So, here is an example.","width":112},{"text":"So, let's assume I have length, and my examples have these values of length, and the corresponding labels are here: negative, positive, positive, and so on.","width":670},{"text":"So, I sorted it, and now I see that it doesn't make sense to split between 15 and 21, but it does make sense to split between 10 and 15, and I'm going to do it uniformly because I don't know better.","width":819},{"text":"Right?","width":32},{"text":"If you had some other domain knowledge, maybe you would do it differently, but without any domain knowledge, I'm going to do it this way.","width":490},{"text":"And now, it's going to be clear that the same algorithm that we developed in the morning is going to work, as is, on this, right?","width":667},{"text":"Okay...","width":117},{"text":"missing values...","width":49},{"text":"so, here is another very realistic, and very interesting scenario.","width":364},{"text":"So, in many situations, you don't have complete examples.","width":396},{"text":"Some of the features, perhaps, are too expensive to measure (say, in the medical domain), and therefore you have examples with some missing values.","width":680},{"text":"And the question is going to be, for us, two fold.","width":182},{"text":"First of all, let's assume we have a dataset with some missing values in the examples.","width":303},{"text":"How do we train?","width":133},{"text":"We have to be able to compute, again, [mumble] where in some of the examples some attribute values are missing, and eventually we will also have to evaluate.","width":836},{"text":"So, here is an example.","width":113},{"text":"Right, so we started growing the tree, we looked at all of the 'sunny' examples, so we started with 'outlook', we chose it, now we are looking at only those examples that satisfy 'outlook=sunny', and we need to grow the tree here.","width":1177},{"text":"This is the set of examples that we have.","width":169},{"text":"We don't know the value of humidity for this example.","width":289},{"text":"What are we going to do?","width":594}],
//   [{"text":"I wanna be able to compute information gain.","width":271},{"text":"In order to compute information gain, remember that I need to compute what I know what Sv is.","width":622},{"text":"I know the sizes and everything, but I need to be able to compute the entropy.","width":306},{"text":"So, for temperature, it's easy for me to do, right?","width":177},{"text":"Because I see, okay, I have two...","width":264},{"text":"twice, I have 'hot', and it's only 'no'.","width":210},{"text":"Right?","width":17},{"text":"Entropy=0.","width":137},{"text":"Then 'cool=yes' has Entropy=0.","width":253},{"text":"And for mild, I have a split, between no and yes, so entropy=1.","width":322},{"text":"So that's easily the computation of the entropy relative to temp.","width":372},{"text":"Okay?","width":29},{"text":"So you agree that what I did here is okay?","width":198},{"text":"It's .97, which was my original entropy, and then 0 times something...","width":536},{"text":"sorry, 0 for the 'hot', and the 'cool', and in two fifths of the cases, it's 1.","width":584},{"text":"Now what do I do with humidity?","width":234},{"text":"Yeah?","width":9},{"text":"[student answers, inaudible since no mic] Okay, what would be the possibilities?","width":928},{"text":"Right, so that's...[mumble]...Okay, so an even earlier suggestion, simpler suggestion: guess a value.","width":439},{"text":"You know, say that it's high, and compute.","width":257},{"text":"Or, in principle, here, I have two highs, and two normals.","width":388},{"text":"If I had three highs and one normal, maybe I could say: I'm choosing high.","width":389},{"text":"Your suggestion is actually more sophisticated, and says, uh, really, I don't necessarily have to have labels that are integers.","width":839},{"text":"I could say, this label is half 'high', and half 'normal'.","width":357},{"text":"In the computation of information gain, no one cares about using integer values.","width":333},{"text":"Right?","width":18},{"text":"It's an entropy computation.","width":112},{"text":"So, so this was my first suggestion: filling in.","width":245},{"text":"Just assign the most likely value, which, in this case, it's a toss up.","width":335},{"text":"So I'm going to choose something, and then the entropy is going to be: three fifths of one of them, and two fifths of the other, or...","width":801},{"text":"the suggestion of the average is equivalent to saying: assume fractional counts.","width":355},{"text":"Right?","width":15},{"text":"I'm going to have two and a half that are 'high', and two and a half of the five that are 'normal'.","width":713},{"text":"Okay, so these are two options.","width":141},{"text":"Any other options that you can think of?","width":296},{"text":"Yeah?","width":8},{"text":"[student answers, but not with a mic, so nobody hears it] Okay, I can do that.","width":372},{"text":"Right, so that's another option.","width":229},{"text":"What do you think should be the right value here of 'humidity'?","width":473},{"text":"So, so far we guessed...","width":87},{"text":"we had two options.","width":153},{"text":"We said: just guess based on majority, and in this case it's a toss up because it's the same.","width":396},{"text":"Another suggestion, we said fractional: let's average between the two, so let's split them evenly.","width":628},{"text":"If you had to guess...","width":110},{"text":"you had to bet on this, what would you choose as the value for humidity here?","width":541},{"text":"Yeah?","width":150},{"text":"We have one vote for high.","width":82},{"text":"Why high?","width":102},{"text":"[student answers, not audible, then laughter] No, but that's actually a very good suggestion!","width":547},{"text":"So even less sophisticated: just look at the correlation with the label.","width":363},{"text":"Right?","width":36},{"text":"So these two 'high' say 'no', these two 'normal' say 'yes', and this is 'no'.","width":522},{"text":"Why won't you guess high?","width":212},{"text":"Any other suggestion?","width":168},{"text":"Or any other justification?","width":159},{"text":"Yeah?","width":9},{"text":"student: my suggestion is: if we guess that it's high, are we enforcing some kind of bias?","width":612},{"text":"Yes, we are.","width":132},{"text":"But is this good, or bad?","width":290},{"text":"Okay, it's a good point, yeah?","width":117},{"text":"[student answers inaudibly] .", "width":630}],
//   [{"text":"Excellent!","width":280},{"text":"So this is actually what you did, and I did it in a simpler way, by just looking at the label, but in both cases what we've done is that we 'did learning'.","width":658},{"text":"We learned the value of humidity as a function of the rest of the data, right?","width":472},{"text":"So, we kind of defined for ourselves a toy learning problem forget that we are in the middle of running an algorithm.","width":626},{"text":"We have a missing value, let's define a toy learning problem: predict this value, assume that it's there, and continue.","width":635},{"text":"Yeah?","width":13},{"text":"[student asks question inaudibly] Yeah?","width":217},{"text":"Say it again?","width":56},{"text":"[student asks question, inaudible] Oh!","width":274},{"text":"That's another idea, let's get to it in a second, or in fact, let me detour to this.","width":449},{"text":"Another option that we should have mentioned is: we can use question marks, or...","width":321},{"text":"three question marks here, as another label, 'unknown', and learn with that.","width":391},{"text":"And again, if we have a lot of unknowns, and we can make an assumption that sometimes holds: that the unknowns have a reason.","width":707},{"text":"Maybe it's a consistent reason, then, maybe we can learn as a function of unknowns also, so that's also a good suggestion, but I want to focus again on this suggestion that we discussed here, so [stammer] while I'm learning, I'm defining my own SPECIAL learning problem, and the goal is to predict what should be here, and I have a lot of information, right?","width":1565},{"text":"I have a lot of examples, typically we think about these as the labels.","width":295},{"text":"Forget it for a second and think about THIS as the label.","width":286},{"text":"Try to predict it, and then guess, and this is actually going to be an important algorithm that we're going to learn later in the semester, in the second half.","width":664},{"text":"We're going to put everything in the probabilistic context, so that our assumptions are going to be a little bit stronger, and this is an algorithm that we're going to call EM: expectation maximization.","width":779},{"text":"Right, so...","width":49},{"text":"you're going to learn something, and in the middle, you're going to try to predict how to complete missing values, put it there, or maybe put the distribution over it if you're not sure, and keep on learning.","width":1018},{"text":"So here you see a very simple instance, but hopefully the intuition is there, and it makes a lot of sense.","width":478},{"text":"Okay, so we talked about how to learn, in the context of missing values, and we agreed that there are many options.","width":471},{"text":"And different options will actually be applicable in different cases.","width":240},{"text":"But we also talked about a principled way, this idea of learning the value of humidity as an internal step of the algorithm is going to be a very important principled algorithm.","width":833},{"text":"Okay, so we talked about the training, but we should do the same thing in test, right?","width":258},{"text":"So now, I've learned a model, I have the tree, you give me an example but, one of the values is missing, so I need to be able to predict.","width":598},{"text":"E.g on this example.","width":223},{"text":"So you know that it's sunny, hot, I don't know humidity, what is the label?","width":395},{"text":"What would you predict?","width":463},{"text":"Yeah?","width":57},{"text":"[student answers, inaudible] Okay, we can do probability, or we can just assume that it's uniform, say.","width":680},{"text":"So, for example, here I have to decide between normal and high, right?","width":373},{"text":"So if I go with 'outlook', it's sunny, so went here.","width":260},{"text":"I don't know what the humidity is.","width":135},{"text":"But I have two options, so I'm going to choose randomly between them, or maybe I can go back to the data, and look at how many examples contributed to each one.","width":788},{"text":"Or, if I have...","width":90},{"text":"this is a tougher example because the root value is missing, 'outlook' is missing, what I can do is that here, I can say 'you know what?","width":506},{"text":"I have three options'.","width":87},{"text":"With probability one third I'm going this way, with probability one third I'm going this way, with probability one third I'm going this way, so what I'm getting is a third are 'yes', a third are 'yes', and a third are 'no', so I'm going to say 'yes'.","width":983},{"text":"And again, I assume that it's uniform.","width":209},{"text":"I could actually assign some weights, in a more sensible way, if I wanted to.","width":330},{"text":"But the idea is that actually there are sensible ways for dealing with it, irrespective of where the missing value is.","width":661},{"text":"Okay, uh, let's keep this, and talk about a couple of other issues.","width":368},{"text":"I'm not going to give details on this, but I just want to mention that it exists.","width":280},{"text":"One issue is that sometimes you have attributes that have different importance- different costs.","width":470},{"text":"So some attributes you feel are more important, and the questions is: how would you change your learning algorithm so that it takes this into account?","width":900},{"text":"And you can change the definition of information gain to account for it.","width":457}],
//   [{"text":"One instance of this is an example that I actually hinted on earlier today, when I asked you, 'what is the entropy?', and I moved from a two value distribution to eight values, and you saw, surprisingly, that actually the maximum value of entropy changes if you add more values.","width":1313},{"text":"That actually indicates that if you have, in the same dataset, attributes with a variable number of values (some will take two values, some will take eight values), maybe you want to fix it somehow, because the information gain values, and the entropy values are going to be in different ranges.","width":1523},{"text":"That's another extension, that you want to think about alternative measures for attributes, as a function of how many values attributes take.","width":831},{"text":"Now number three, here, is the notion of oblique decision trees.","width":283},{"text":"The decision trees that we have now, you can think about them (and I showed an example earlier), as axis-parallel decision trees.","width":591},{"text":"Let's take the case of continuous variables.","width":251},{"text":"You ask, 'is A less than C, or greater than C?","width":390},{"text":"Is B less than something or greater than something?' You could imagine that rather than splitting on individual attributes, you can split on a combination of attributes.","width":891},{"text":"So you have two attributes, A and B, and you want to split on whether 2A+3B>5, or not.","width":562},{"text":"So essentially you define a hyperplane, a linear combination of features, and you put this in your node, and split on that.","width":760},{"text":"This type of decision tree is going to be called an oblique decision tree.","width":319},{"text":"[student asks question, inaudible] It is, it's transforming the feature space, but I wanna learn the decision tree with this transformed space, and I have two options.","width":1509},{"text":"One option would be: let's first define a new feature space, and then learn the decision tree, but then it's uninteresting, because I actually have decoupled these two steps.","width":733},{"text":"I generate a new feature space, and I use my training algorithm.","width":249},{"text":"Another potentially more interesting option, is: can I figure out, what are the right combination of features?","width":485},{"text":"Whether it's 2A+3B>5, or 3A+2B>5, as I'm learning.","width":528},{"text":"Essentially, find the right hyperplanes as I'm learning.","width":267},{"text":"There are algorithms that support this, where the decisions are not axis-parallel, and you can learn oblique decision trees.","width":643},{"text":"Finally, the last comment I want to make here is that you may want to learn this incrementally.","width":381},{"text":"The way we described the algorithm now is: here is the data.","width":352},{"text":"That's it, that's the training data.","width":100},{"text":"Learn.","width":86},{"text":"But you could imagine streaming data.","width":204},{"text":"So, data comes in as you learn, you learn a decision tree based on some of the examples, and new examples come in.","width":559},{"text":"This problem is a little bit harder, because maybe the training data is inconsistent with the tree that you've learned, so updating it incrementally could be another problem.","width":1414},{"text":"Again, this is a problem that people have looked at, and developed algorithms that do this.","width":407},{"text":"I'm not going to get into all these details, if specifically, you care about decision trees, you can look at the literature.","width":529},{"text":"I want to spend a little bit of time on one very very important application of decision trees: decision trees as features.","width":617},{"text":"Someone asked me a question at the end of class about this, and you will understand this completely when you do your homework two, because you will have to use decision trees as features, but I want to give you the key idea, and some examples.","width":1080},{"text":"The idea is that rather than using decision trees to represent one target function ('here is data, learn a tree'), I'm going to use the same algorithm to learn multiple small trees, and use them as features.","width":1121},{"text":"Learning one big decision tree often results in overfitting, because it's a big decision tree.","width":518},{"text":"All of the reasons that we discussed earlier.","width":201},{"text":"What I'm going to do instead is learn small decision trees, with limited depths, and I'm going to say, I'll only learn depth 2, and that's it.","width":690},{"text":"Now, the first question you asked yourself is: 'what do you mean, you learn multiple decision trees?'.","width":456},{"text":"You have one algorithm, you have one collection of data points, the algorithm is going to produce one decision tree.","width":546},{"text":"What do you mean when you say 'learn many small decision trees?'.","width":432},{"text":"What do I mean?","width":212},{"text":"[student answers, inaudible].","width":357}],
//   [{"text":"That's what I am gonna do.","width":361},{"text":"I am not gonna look at all the data.","width":179},{"text":"I am gonna look at subset of the data or reweighting of the data.","width":318},{"text":"somehow.","width":25},{"text":"and I am gonna generate, so, say I have 1000 examples, I am gonna sample every time 100 examples.","width":537},{"text":"learn small decision trees for me.","width":568},{"text":"(Student asking questions...","width":567},{"text":"very unclear.) So another option would be, you have n features.","width":588},{"text":"Learn every time, 3 or some subsets of n features.","width":342},{"text":"choose half the features.","width":153},{"text":"learn decision trees on this.","width":153},{"text":"choose randomly n over 2, learn another decision tree.","width":275},{"text":"Choose another subset of the features, learn a decision tree.","width":183},{"text":"so the multiple ways is a way, the goal is to generate multiple decision trees.","width":414},{"text":"the corresponse to some aspect to this data.","width":226},{"text":"right?","width":16},{"text":"so that's what i mean by learning multiple decision trees.","width":229},{"text":"and i am gonna think about them as experts.","width":196},{"text":"basically, that means it's a mapping, from data to 0 or 1.","width":492},{"text":"OK.","width":64},{"text":"now, i am gonna learn another function, with this features.","width":478},{"text":"think about it.","width":112},{"text":"essentially, what i did is i did transformation of the feature space.","width":256},{"text":"let's assume i started with 1000 examples, with 50 features each.","width":451},{"text":"ok.","width":75},{"text":"so my examples are 50 dimensions, a bit vectors.","width":439},{"text":"now i am gonna learn, say, 100 decision trees.","width":302},{"text":"so i am take 1000 examples, and producing 100 decision trees.","width":358},{"text":"each decision tree have the following property knows how to take an example and produce 0 or 1.","width":488},{"text":"positive example or negative example.","width":195},{"text":"so i can take now each example, and convert it to 100 dimensional vector, which is what each decision tree says on this example.","width":660},{"text":"I took an example and I ask the view of all my 100 experts, what do you think about this example.","width":609},{"text":"Then I am gonna get 100 views and that's gonna give me a new presentation of this example.","width":448},{"text":"this presentation just indicates what my current collection of decision tree think about it.","width":425},{"text":"so now i replace the 1000 example that i had before with another 1000 examples.","width":473},{"text":"the dimensions mean different things now, but i don't care.","width":306},{"text":"I can take this collection and send it to another algorithm, and typically, i am gonna send to a linear function, like the gradient descent or the stochastic gradient descent, that we talked about last week.","width":931},{"text":"and that's gonna be my learning.","width":220},{"text":"so that's actually, basically the problem said you are gonna do next time, you are not gonna learn, you are not gonna write the algorithm for decision tree, you are gonna use one.","width":724},{"text":"I am gonna you an example that is really important.","width":195},{"text":"1 of the way to combine decision tree is an algorithm that called boosting, that we are gonna learn in a few weeks, in the middle of the semester.","width":700},{"text":"really important algorithm, with lots of cool properties.","width":287},{"text":"let's see.","width":51},{"text":"ok.","width":126},{"text":"here is an example.","width":193},{"text":"I am not gonna get into the mathematical details, because we have a few more weeks before we get there.","width":1475},{"text":"but....","width":513}],
//   [{"text":"[Gibberish].","width":423},{"text":"Okay.","width":578},{"text":"Okay that is better.","width":461},{"text":"SO umm, here is the dataset that I have.","width":246},{"text":"I have five blue positive examples and four negative examples.","width":460},{"text":"And I want to learn a classifier.","width":301},{"text":"So the first thing I am going to do and you can see that the classifier is kind of complicated.","width":375},{"text":"YOu can't see a simple way to split positive from negative.","width":307},{"text":"What I'm going to do is learn simple decision trees.","width":199},{"text":"Here is my first decision trees.","width":167},{"text":"This decision tree says if you are on the left of this vertical line.","width":409},{"text":"Say positive.","width":60},{"text":"If you are on the left then say negative.","width":214},{"text":"It is a legitimate decision tree the mistakes are circled here.","width":413},{"text":"But it is doing okay on the what is this, six of the examples.","width":502},{"text":"So that is my first decision tree.","width":112},{"text":"This algorithm boosting involves rescaling the examples.","width":300},{"text":"You can see some of the pluses and minuses change size.","width":237},{"text":"But we won't care about this now.","width":123},{"text":"The important thing is that we have one decision tree.","width":178},{"text":"Next, I In round two I am learning another decision tree.","width":341},{"text":"In this case this decision tree says look at this line.","width":245},{"text":"So this is a one node decision tree.","width":173},{"text":"Very simple decision tree.","width":168},{"text":"If it is on the right say negative otherwise say positive.","width":287},{"text":"Again you see that I make the mistake.","width":188},{"text":"I got six example lines.","width":253},{"text":"Second decision tree.","width":147},{"text":"Then I am going to suggest another decision tree.","width":206},{"text":"In this case that is on the line.","width":147},{"text":"If I am above the line say positive otherwise negative.","width":301},{"text":"Another decision tree, one node decision tree.","width":197},{"text":"And I am going to make two mistakes.","width":182},{"text":"And you can see that there is some numbers here.","width":121},{"text":"Next to indicate how I'm scaling this data.","width":195},{"text":"But I won't get to this now.","width":178},{"text":"But the important thing is that now I can take this decision tree.","width":298},{"text":"I'm putting some weight next to it.","width":121},{"text":"It is a linear function.","width":196},{"text":"The coefficients are here.","width":92},{"text":"The coefficients you can hardly read it but it is .42, .65, .92.","width":450},{"text":"And it turns out if you take these decision trees with these weights, you get a perfect classifier.","width":381},{"text":"It classifies positive and negative.","width":292},{"text":"So that is an example of an algorithm that uses simple decision trees as features.","width":479},{"text":"And combines them using a linear classifier.","width":192},{"text":"Now what I did not tell you is how I decided which trees to learn.","width":490},{"text":"Right I put a vertical line here, I could have put it somewhere else.","width":201},{"text":"I chose to put it there.","width":121},{"text":"And I also didn't tell you how I learned the weights for the tree.","width":299},{"text":"Right?","width":15},{"text":"So if I don't put here these numbers or change them significantly I probably won't get this nice hypothesis.","width":597},{"text":"But I could learn, there is a way to learn these weights and get correct.","width":354},{"text":"The key idea that I want to share with you is that we can use decision trees, in fact today the key use of decision trees is this way.","width":1032},{"text":"Learning features, the features could be expressive.","width":204},{"text":"In this case we showed you one node.","width":134},{"text":"But if you take this decision tree that is a little bit larger a few levels.","width":320},{"text":"It could be a very expressive transformation of the original features.","width":354},{"text":"Then I'm going to use these as features and learn from them.","width":306},{"text":"Questions?","width":274},{"text":"So is this clear when I say use decision trees as features?","width":515},{"text":"Basically a transformation of this space.","width":144},{"text":"Okay, so let's finish here with .", "width":1243}],
//   [{"text":"Okay.","width":151},{"text":"Okay.","width":14},{"text":"I want to say a few things about experimental machine learning.","width":277},{"text":"I am actually not going to talk about all of the details in the slides because it is probably better for you to go to these slides once you see the next problem set because you are going to do it.","width":633},{"text":"I am just going to talk about it in a general way and we will see.","width":392},{"text":"Machine learning is an experimental field with a lot of theories, but as I said the first three or four problem sets are really involved with experiments and it is important for you to spend some time learning how to experiment.","width":1128},{"text":"The bottom line is, you have to be organized, you cannot expect that you can sit in front of your terminal and run everything.","width":492},{"text":"You have to write scripts that would run everything for you and you log everything so that you know what you have done and what you haven't done.","width":650},{"text":"You wake up in the morning and you see what happened.","width":173},{"text":"So that really the key lesson.","width":244},{"text":"But there is a little bit more issue in terms of the methodology of finding experiments and some of the basics are: you have to split the data into two or three data sets.","width":806},{"text":"Basically we want to have training data, we want to have test data, and we want to have some development data, or validation set, that you use in order to make some decisions.","width":868},{"text":"This decision can be, when do I stop training my algorithms.","width":264},{"text":"Or this decision can be, this is how I want to tune my parameters if there are parameters.","width":584},{"text":"You need to report the performance on test data, but your test data is something that you are not allowed to look at.","width":473},{"text":"You look at training and you look at development.","width":244},{"text":"You can already see that there is a bug in this algorithm, this methodology.","width":321},{"text":"This is a one time process.","width":109},{"text":"If you think about it from the perspective of the research community.","width":223},{"text":"In the first time this data set is being used, it works.","width":213},{"text":"Second time, maybe.","width":100},{"text":"After many times, of course people tune on the test data, because this is the result they publish.","width":391},{"text":"Even though they dont do it directly, that is what they compare.","width":293},{"text":"So there is some bug in this process, and there is actually research on trying to do this right.","width":369},{"text":"But that is the methodology we are going to use.","width":255},{"text":"That is the commonly used methodology.","width":239},{"text":"So specifically, there are some subtleties or some modifications to this.","width":402},{"text":"So instead of just splitting the data to train and test: single test training split.","width":464},{"text":"You can split the data to n equal parts.","width":216},{"text":"The one that is commonly used for different statistical reasons is five fold cross validation, which means you split your data to five parts, same size, everything you train on 80%, four of the folds, and test on the other, the fifth one.","width":1339},{"text":"And then you report the average accuracy, or whatever metric that you need, on this five fold you can also report the standard deviation.","width":659},{"text":"So that is the key methodology.","width":163},{"text":"The next few slides talk about how to measure significance of your results.","width":437},{"text":"So if you compare two algorithms and you get for one algorithm 85% and for the other one 86% or 95%, the question is, is this significant or not.","width":856},{"text":"Intuitively, you want to say, if you give this one a lot of tests and a lot of data, it is significant, if you didn't do it on a lot of data, it is probably not significant.","width":647},{"text":"But there are statistical test to actually quantify this, and the next few slides provide a few recipes that are easy to implement.","width":670},{"text":"You will implement when you report your results.","width":197},{"text":"I am not going to get into it now because it would be a little bit out of context before you see the question.","width":402},{"text":"I think I am going to stop here and answer some questions.","width":232},{"text":"Ques .", "width":293}],
//   [{"text":"Okay so let's start.","width":222},{"text":"I see that a few of you decided- hopefully it's not because of the problem set that is due- to not come today, so as a reward I'm gonna give you some hints on the problem set.","width":1190},{"text":"So anyhow so a few administration things: registration I think as of this morning there were eleven empty slots so a few of you should have received emails that you can register.","width":1128},{"text":"If you have the forms here with you, come immediately after the class, and I'll sign it.","width":391},{"text":"If not, we'll do it next time.","width":378},{"text":"So but but we're going down the waiting list and it looks good.","width":383},{"text":"Partly I think due to the fact that some of you thought that homework one was too difficult.","width":552},{"text":"I wanna say two things.","width":237},{"text":"First I'm going to talk a little bit later today about something that is tightly related to homework one, but I wrote a comment this morning on piazza, and the comment was meant to say something that I said during the first lecture that I'm viewing the homework as an extension of the class.","width":1130},{"text":"So my goal is that you will learn something from doing the homework.","width":271},{"text":"So it's not going to be just 'I said it in class now you're going to say it on paper' because that's going to be a waste of time for all of us.","width":636},{"text":"But rather building on concepts that we've learned in class, the idea is that we will guide you in learning new things, and the goals is gonna be that this is gonna be the case in all the homework.","width":1048},{"text":"You will learn different things, some are theoretical, some are experimental, some are methodological in each of the problem sets, but the idea is that it's not going to be exactly what you've done in class, or maybe a small part of it will be.","width":884},{"text":"The rest of it will be guiding you learning new things, building on what we've done in class.","width":432},{"text":"The quizzes, on the other hand, are essentially what we've done in class, and the idea is to help you make sure that you understand and that you kind of look at the material before the class.","width":969},{"text":"If you don't have it on the top your head, so that's gonna be the idea.","width":603},{"text":"Homework two is gonna be out tonight once you upload homework one.","width":471},{"text":"And as I've said before, please start working on it as soon as you can.","width":405},{"text":"I mean don't wait with the tough question to the day before it's due because it's unlikely you'll make it if you start the day before.","width":834},{"text":"So and please come to the discussion sections with questions to the office hours and so on.","width":471},{"text":"I'm sure many of you will have questions.","width":375},{"text":"Okay, so that's now.","width":101},{"text":"So second part: the videos are now online, available, open to everyone, including people not yet registered, so I don't know if this is good or bad, but it's out there.","width":952},{"text":"One thing is that because we are putting it online, we are we have to add captions.","width":719},{"text":"That's a law, and even though we are behind because we are a few weeks behind, we need volunteers to help us with captioning the lectures.","width":727},{"text":"It's not going to be completely volunteering: we're going to give you some extra credit for captioning.","width":444},{"text":"There's gonna be some details on exactly how to do it, there are some tools for doing it.","width":340},{"text":"And it will actually be good for you I think because you definitely understand what you're captioning, you'll have to go over it in details, so that will be good.","width":622},{"text":"That was part 2.","width":168},{"text":"Part 3: no lecture on Thursday because I'll be away and I will not have office hours this week.","width":613},{"text":"The TAs will have office hours and discussion sections so please us it.","width":458},{"text":"Questions?","width":209},{"text":"Yeah.","width":8},{"text":"[Student].","width":220},{"text":"Yeah, there will be discussion sections all this week yeah.","width":291},{"text":"Everyday there will be one discussion section.","width":370},{"text":"Including Friday.","width":192},{"text":"Right?","width":43},{"text":"Yeah, so any other questions?","width":592}],
//   [{"text":"Yeah, okay, so maybe I was...","width":107},{"text":"it was confusing.","width":61},{"text":"Just to make sure, there's no lecture on thursday, but all the rest- office hours of the TAs, discussion sessions, this goes as usual this week.","width":565},{"text":"Yeah, there was a question there.","width":218},{"text":"[student asks question, inaudible] I don't know what the difference is.","width":612},{"text":"It's one point, which is one percent.","width":306},{"text":"So you will see that it slides to five minute pieces, and you have to do something like fifteen pieces in this software package to accumulate one lecture.","width":853},{"text":"It doesn't have to be consecutive if you don't want it to be.","width":222},{"text":"[student asks question, inaudible] I think the number is fifteen...","width":619},{"text":"something like that.","width":88},{"text":"But, you know, if you want to do a lot, please.","width":461},{"text":"I guarantee that you will understand everything once you caption enough lectures.","width":524},{"text":"Okay, so I want to talk a little bit about a few other things that it's time to start talking about.","width":477},{"text":"One of them is the project.","width":112},{"text":"So, about fifty of you, plus or minus a few, are registered for four hours, and I want to talk about this, and a few of you (quite a few), actually told me that they want to do these four hours...","width":853},{"text":"or...","width":52},{"text":"the project, even though they are not registered for four hours.","width":253},{"text":"So, the way we are going to do it is: there is going to be a team project, ideally two to three, and the projects are due at the end of the semester, a few days after the final exam, but I want you to write a short proposal, ideally one page, definitely no more than two pages.","width":1428},{"text":"There are some instructions on the web on what I want to see in this proposal.","width":356},{"text":"Essentially I want to know what the project is about, and I want you to know something about what has been done in the literature on this topic.","width":909},{"text":"A mandatory part of the project is that you have read at least two papers that are relevant to the topic of your research, or it could also be two chapters, if it's not a current topic.","width":1162},{"text":"Write a proposal, send it to us, we're gonna give you more instructions on how exactly to do it, and you will get feedback on: excellent project, change this and that, or maybe change the project, which is unlikely.","width":1176},{"text":"Hopefully everything will be good.","width":121},{"text":"Really, start thinking about it.","width":229},{"text":"We're talking about just a few weeks from today.","width":367},{"text":"Everything that has a significant machine learning component is okay.","width":302},{"text":"It can be a theoretical paper, it can be an experimental paper, it can be a combination, it can be a survey paper of some topic that you want to know more about, but as I said, it has to include some reading.","width":799},{"text":"I also say that originality is not mandatory, but is encouraged.","width":384},{"text":"Really try to make it interesting.","width":111},{"text":"Every year there are a few projects that are really, really interesting, and a couple of projects that are on the verge of being published, with a little bit more work, so it's possible.","width":858},{"text":"Here are some examples for topics.","width":249},{"text":"In no way are they a mandatory, complete, or exhaustive list, just some ideas.","width":387},{"text":"So one idea, you can look at competitions that ran in the literature, or that are running now.","width":465},{"text":"KDD cup is one example.","width":142},{"text":"This was a nice topic of author-paper identification.","width":224},{"text":"You can look at kaggle.","width":233},{"text":"They always have a lot of competitions.","width":316},{"text":"You can look at one or several of them, and see if this is something you are interested in.","width":354},{"text":"Here is an extension of this KDD cup thing that I call author profiling.","width":439},{"text":"Given a set of documents- it could be papers, could be books, could be any other...","width":348},{"text":"blogs- try to say something about the author...","width":317},{"text":"gender identification, native language.","width":255},{"text":"If there are multiple authors, this problem becomes really hard.","width":319},{"text":"See if you can have some idea of what to do with multi-author papers.","width":294},{"text":"So this is one interesting direction.","width":246},{"text":"Caption Control.","width":53},{"text":"So we just talked about you captioning the lectures.","width":279},{"text":"You can think about some malicious people- none of them are sitting in this room- that will not actually caption the content but rather, write some gibberish, or spam in this.","width":923},{"text":"Can you write a program that will run over the caption, and identify this.","width":267},{"text":"So you can think about gibberish detection, spam detection.","width":214},{"text":"You can think about determining the quality of this captioning, or any other text for that matter, but it can be done in the context of the caption control.","width":776},{"text":"You can think about working on making learned hypotheses more comprehensible.","width":446},{"text":"We talked about the linear threshold function, we're gonna talk more about it.","width":210},{"text":"We're gonna talk about neural networks.","width":137},{"text":"Can you do something that will serve as a good example for why you made this prediction?","width":455}],
//   [{"text":"Umm, You can develop a people identifier.","width":325},{"text":"A person identifier.","width":96},{"text":"And you can think about the multi model while doing it.","width":207},{"text":"By could be depending on the modality you exposed to.","width":220},{"text":"Maybe a combination of, you want to be to develop a robust people identifier.","width":338},{"text":"You think about it that there is a sensory myopis.","width":198},{"text":"You know who is coming in my door without seeing it.","width":365},{"text":"So could be any other modality that you can think of can be video, can be audio.","width":595},{"text":"The way they work.","width":98},{"text":"All interesting machine learning project.","width":186},{"text":"Compare regularization methods.","width":100},{"text":"We are going to talk about it later this semester, but some of you already know something about.","width":351},{"text":"Some of you will know something about it in the next couple of weeks.","width":177},{"text":"You can think about comparing different methods for example L1 regularization that we are going to briefly mention vs winnow algorithm.","width":561},{"text":"They are doing something conceptually related.","width":230},{"text":"This would be interesting to compare experimentally theoretically.","width":418},{"text":"And few other things that I mentioned here all of these are suggestions if you have ideas feel free to come not this week, but in the next office hours.","width":893},{"text":"To talk to me about it.","width":120},{"text":"To talk to the TAs.","width":187},{"text":"and basically in a few weeks, you will arrive at the project.","width":316},{"text":"Questions?","width":335},{"text":"No questions?","width":41},{"text":"Okay.","width":58},{"text":"So let's go back to the lecture so.","width":273},{"text":"So this is where we are in class.","width":101},{"text":"We talked about learning algorithms.","width":115},{"text":"We started with a search algorithm.","width":201},{"text":"We talked about gradient descent and stochastic gradient descent.","width":211},{"text":"We are going to talk about this more this class.","width":224},{"text":"We talked about decision trees.","width":91},{"text":"We did not talk about rules because there is no time, but similar ideas to the perspective of learning.","width":515},{"text":"We also talked about the importance of the hypothesis space when we talked about linear classificaiton we are going to talk about this issue more.","width":587},{"text":"So, there are two issues that we haven't talked about that we have already highlighted as important that we are going to talk about today.","width":515},{"text":"How are we doing?","width":115},{"text":"Can we quantify what we are doing?","width":195},{"text":"We are going to start today of the simplest way of talking about it.","width":352},{"text":"And reach this a little bit later.","width":142},{"text":"We are going to start talking about one of the most important learning algorithms that you will see in this class and general, perceptron and a lot of variations on this.","width":803},{"text":"And including multi-layered perceptron.","width":317},{"text":"And moved beyond binary classification and talk about more general ways to quantify performane of learning.","width":706},{"text":"This will be the end of the first part of this class.","width":237},{"text":"so today we are going to talk in a more general perspective on learning in general.","width":472},{"text":"Think about learning protocols quantifying performance and motivate some algorithmic ideas that we are going to start seeing again by the end of today.","width":676},{"text":"Okay, so.","width":130},{"text":"So what do I mean when I say quantifying performance.","width":145},{"text":"I want to be able to say something rigorous about the performance of the learning algorithm.","width":331},{"text":"So far you know a few algorithms already.","width":255},{"text":"You can run them on your dataset and how they are doing on this dataset but that is it.","width":504},{"text":"And clearly this is not what we want.","width":279},{"text":"So today we are going to talk about one narrow perspective simple but you can see that this is quite general.","width":623},{"text":"We are going to talk about quantifying performance as a number of examples that we see before we can say something about the learned hypothesis.","width":640},{"text":"So let's start with the following example.","width":317},{"text":"This is learning conjunctions.","width":177},{"text":"It is not the same problem you have in your homework.","width":243},{"text":"But, conceptually it is quite related.","width":280},{"text":"So I'm going to assume that there is a hidden monotone conjunction-- I mean no negated variables okay?","width":589},{"text":"You also know how monotone functions from other contexts.","width":443},{"text":"What is a monotone function in real analysis or calculus or any other class we would take?","width":610},{"text":"What is a monotone function?","width":111},{"text":"You have seen this notion I hope.","width":243},{"text":"Yeah .", "width":345}],
//   [{"text":"Great, so, so basically, again, you are using the fact that it is a monotone conjunction and you are using the fact that you know it.","width":539},{"text":"And you know that there are exactly six, uh, on variables.","width":309},{"text":"And you are saying first of all I am gonna give you an example that has 1 in exactly the six places, actually these five places.","width":615},{"text":"And I'm gonna, oh, this is of course, a positive example.","width":174},{"text":"What does this example teach you?","width":98},{"text":"[Student answer: Inaudible.] This is a super set of the important variables.","width":655},{"text":"Right?","width":35},{"text":"All those that are 1s could be there.","width":207},{"text":"Not all of them have to be there, but none of the 0s could be in the conjunction.","width":319},{"text":"That's important, right?","width":125},{"text":"You have to understand this, none of the 0s could be there because this is a positive example.","width":331},{"text":"Okay?","width":120},{"text":"So now, now that I have a super set, one by one, I can show you that each one of these variables is actually necessary.","width":578},{"text":"So I am gonna, uh, to show that is necessary, I am gonna drop, once we are here I drop x2.","width":455},{"text":"And I say, well, this is a negative example, which means x2 is necessary.","width":296},{"text":"And then I am gonna drop x3.","width":165},{"text":"The example is negative.","width":101},{"text":"I need x3 and so on.","width":128},{"text":"So, how many examples do we need here?","width":200},{"text":"Six examples, one for each bit plus the super set.","width":283},{"text":"So you see a good teacher actually can save you a lot of time, by teaching you the right thing.","width":493},{"text":"Uh, if you have the right algorithm of course.","width":184},{"text":"And, by the way, this is an interesting protocol.","width":219},{"text":"Uh, and still an interesting research question in machine learning.","width":279},{"text":"Uh, what's the value of a teacher?","width":163},{"text":"How to develop good teaching algorithms as oppose to only good learning algorithms.","width":440},{"text":"Uh, still, uh, an active research question.","width":372},{"text":"Okay, so uh, and by the way modeling teaching, as I said in the beginning, is tricky, because you have to make sure there is no collusion here.","width":592},{"text":"Okay, so let's move to the third protocol.","width":168},{"text":"And the third protocol is the one that is mostly studied in machine learning, uh, partly because it's more natural and partly because it's easier to analyze.","width":806},{"text":"Uh, and this is the one you had in mind before.","width":278},{"text":"And this is, kind of, the one that we are going to study the most of the semester.","width":286},{"text":"There is some random source.","width":218},{"text":"Let's call it nature that generates training examples.","width":191},{"text":"And the teacher, that knows the label, knows the function, labels them.","width":347},{"text":"So what I am going to see is I am going to see a series of random examples, with labels.","width":315},{"text":"And my goal is to use this in order to learn the conjunction.","width":382},{"text":"Okay, so, algorithm?","width":170},{"text":"What would you do?","width":561},{"text":"You already thought about a similar problem so you probably know how to do this.","width":430},{"text":"Yeah.","width":8},{"text":"[Student answer: The intersection of all the positive examples.] Okay, so what will that give me?","width":442},{"text":"[Student answer: Inaudible.] A super set of variables.","width":284},{"text":"So that's actually a good idea.","width":147},{"text":"I'm gonna present it as an online algorithm, instead of this.","width":306},{"text":"So I am not going to assume that I have all the positive points in the dataset ahead of time, instead I'm gonna present them one by one.","width":545},{"text":"They come one by one.","width":91},{"text":"So we will have to modify your algorithm a little bit, so that it will work in the online way.","width":469},{"text":"So how would we change it?","width":417},{"text":"This is still a good idea so you can build from that.","width":281},{"text":"Online algorithm, I am giving you an example.","width":135},{"text":"Yeah.","width":156},{"text":"[Student answer: Inaudible.] So happen that it is.","width":329},{"text":"[Student answer.] Everyone agrees with that?","width":1062}],
//   [{"text":"With high probability, at least 1 minus delta.","width":243},{"text":"So with high probability, it makes a small error.","width":230},{"text":"Here I'm just saying how many mistakes we're gonna make before we stop making mistakes completely, zero error.","width":509},{"text":"It turns out that it doesn't matter, I mean everything we can say in the simple version, we'll stop making mistakes, we can say the more general and vice versa basically.","width":847},{"text":"So, it's easier to analyze things in the more kind of extreme worst case simple protocol.","width":531},{"text":"Okay, the drawbacks: it's too simple.","width":211},{"text":"But the advantage is also that it's simple.","width":306},{"text":"And we will see that we can actually derive a lot of milage from this.","width":425},{"text":"So the first fundamental question is is it even a legitimate concept?","width":492},{"text":"Is it clear that we can bound the number of mistakes of a learning algorithm?","width":608},{"text":"So think about it a little bit.","width":129},{"text":"Who says that it will stop making mistakes at some point as you learn.","width":323},{"text":"Is it clear to you?","width":65},{"text":"[Student].","width":802},{"text":"True, so for the conjunction we gave this elimination algorithm, every mistake we make, we drop a literal.","width":508},{"text":"There are n literals, we're gonna make no more than n mistakes, but in general is this a legitimate notion.","width":566},{"text":"Yeah?","width":420},{"text":"[student].","width":331},{"text":"Uhh, even that is not clear to me.","width":274},{"text":"So nothing in the learning algorithm- that's actually an important point- in general it's not true that if you algorithm makes a mistakes, makes a mistake on an example, you correct the hypothesis.","width":1122},{"text":"Later on, when it says the same example again, it will not make a mistake on it?","width":319},{"text":"In fact, in most learning algorithm, it will not have this property unless you do something special.","width":432},{"text":"So the number of examples by itself is not going to same you here.","width":312},{"text":"But I will give you a hint, let's take the case where we have a finite concept class.","width":538},{"text":"So the number of target functions is finite.","width":296},{"text":"Conjunction is an example of this.","width":130},{"text":"In fact, all the set of boolean functions is an example of this right?","width":249},{"text":"Two to the two to the n.","width":92},{"text":"Large but finite.","width":75},{"text":"What about this case?","width":199},{"text":"Is it clear that in this case, the finite hypothesis class, you're gonna stop making mistakes at some point?","width":492},{"text":"Why yes?","width":822},{"text":"[student].","width":264},{"text":"Right, so this seems simple, but actually it's quite deep.","width":518},{"text":"So this is the algorithm that you propose.","width":142},{"text":"So, I'm running my algorithm.","width":191},{"text":"In the ith stage of the algorithm, let c_i be the set of all concept in C, all functions that are still consistent with all the data I've seen so far.","width":780},{"text":"Okay, is it clear what I'm saying?","width":140},{"text":"So I have a lot of functions, and I'm taking a subset of all these functions that are consistent with the data I've see so far.","width":550},{"text":"I'm not talking about computation now.","width":105},{"text":"[e.g.] I don't know how to compute it.","width":117},{"text":"But, it's a well defined notion, right?","width":326},{"text":"So c_i is this set.","width":146},{"text":"I'm choosing randomly a function from this set C_i, and I'm using it to predict on the next example.","width":903},{"text":"Clearly, C_{i+1} is a subset of C_i.","width":184},{"text":"And if I've made a mistake on the ith example, C_{i+1} is strictly smaller than C_i, right?","width":598},{"text":"This function that I chose was not is no longer consistent with the data because it made a mistake on one example, right?","width":600},{"text":"So I've dropped it and I have a smaller set.","width":188},{"text":"So this is the algorithm that was proposed.","width":124},{"text":"Basically it says everything time you make a mistake, you get rid of at least one hypothesis, and therefore you've made progress.","width":580},{"text":"So this CON algorithm for consistency makes at most |C|-1 mistakes, and the answer is yes.","width":703},{"text":"Can you do better than this really really simple algorithm?","width":899}],
//   [{"text":"So yeah?","width":85},{"text":"[Student: uh, it depends on what you're calling for.","width":266},{"text":"Uh find the more efficient algorithm in not necessarily 100% accuracy on the test...] No no, I want something that will stop making mistakes, and I'm gonna give you a hint.","width":1110},{"text":"This algorithm is gonna use the most important concept in Computer Science, which is...","width":498},{"text":"what's the most important algorithm in Computer Science?","width":263},{"text":"[Student].","width":165},{"text":"You're in the area: binary search!","width":303},{"text":"How would you use binary search here to give me a better algorithm than this thing?","width":615},{"text":"So here we just eliminated one by one.","width":184},{"text":"How would you use binary search to eliminate faster yes?","width":362},{"text":"[Student].","width":356},{"text":"Half of the hypothesis that produced it.","width":191},{"text":"Exactly, that's the idea!","width":105},{"text":"So let's set it up so that every mistake we eliminate half the hypothesis mistake.","width":344},{"text":"So it's called the halving algorithm and understand why.","width":316},{"text":"The ith stage of the algorithm as before, C_i is gonna be the set of all hypotheses that are consistent with everything else in before, okay?","width":972},{"text":"Now, given a new example, what I'm gonna do is I'm considering the value of each function on this examples of all the functions that are still available to me.","width":790},{"text":"They are consistent with everything I've seen before, and I'm gonna predict by majority.","width":388},{"text":"Right, so I'm gonna look at all those functions that say predict one.","width":278},{"text":"All those functions that say predict zero.","width":154},{"text":"If I made a mistake, I eliminate at least half, agreed?","width":405},{"text":"Again, it's not an efficient algorithm.","width":151},{"text":"I don't know how to compute it, but imagine that you can have access to all the functions in your concept class that are still consistent with the data.","width":628},{"text":"All the data you've seen before.","width":161},{"text":"Evaluate all of them on this new examples.","width":243},{"text":"And do by majority: some of them say predict zero, some of them say predict one.","width":306},{"text":"If you got it wrong, it means that the majority mislead you: get rid of this.","width":469},{"text":"So, if you predict one if the number of functions that say zero say predict zero is smaller than the number of functions that say predict one, and clearly the size of C_{i+1} is less than a half of the size of C_i, so the halving algorithm makes at most log(|C|) mistakes.","width":1672},{"text":"Huge gain relative to the size of C to the log of the size of C, right?","width":420},{"text":"The cool thing about this algorithm that is not computable is that actually we can achieve it with an efficient algorithm in quite interesting cases.","width":861},{"text":"So this just motivates: wow, maybe we can do this!","width":197},{"text":"So I'm gonna give you examples of maybe how we can do it.","width":138},{"text":"So let's looking at learning conjunctions, right?","width":158},{"text":"So, this is the hidden conjunctions: how many conjunctions are there altogether?","width":456},{"text":"If you think about it, in general 3^n because each variable can be there in a positive way, negative way, or not either in the conjunction, so you have n slots.","width":915},{"text":"Each slot you can put positive variable, negative variable, no variable.","width":319},{"text":"3^n.","width":110},{"text":"So log of this is n.","width":248}],
// ];
//
// if (typeof module !== 'undefined') {
//   module.exports = videoCaptions;
// }

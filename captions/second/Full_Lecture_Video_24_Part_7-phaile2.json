[{"text":"Tricky thing about averages you can have outliers, and have your outliers still be nice","width":832},{"text":"So I'm not really making any claims about the worst case, somebody could break things and give you a really bad looking worst case list.","width":512},{"text":"I'm talking about on average what is the running time of a find here?","width":192},{"text":"If you want the worst case, go take algorithms or something and talk to Jeff Erikson","width":448},{"text":"I'm not gonna do that here","width":128},{"text":"So what about remove?","width":128},{"text":"Remove is sort of the same story.","width":128},{"text":"The hard part is locating the element which is going to be in the worst case traversing one of those lists and I've made an argument about the average length of one of those lists.","width":704},{"text":"So this removal is also constant time","width":512},{"text":"I lied, this is amertized as well","width":64},{"text":"Why is insert amertized?","width":128},{"text":"I might have to resize the array","width":320},{"text":"So we've now got a hash table which has given us amertized constant time for all of the dictionary ADT functions.","width":768},{"text":"In practice this is going to be significantly faster than using a tree structure.","width":320},{"text":"The problem is its average","width":128},{"text":"So no complete gurantees here","width":448},{"text":"Yes","width":128},{"text":"Well","width":64},{"text":"The worst case is sort of crap because if you can't do anything about making find or remove faster, there is not any need to resize the table","width":960},{"text":"Technically if you are resizing the table that would have to be amertized","width":448},{"text":"The point of that was more you really need uniform hashing assumption","width":256},{"text":"I care more about this","width":256},{"text":"But you should understand why that happens","width":448},{"text":"There are other resizing strategies you could potentially investigate for this but the problem here is you need to be able to make sure that you can always find the things that you had before when you did the resize","width":1856},{"text":"And you can make an observation that if you try and preserve that old table structure. that old table structure was starting to get bad","width":640},{"text":"When you do the resize the length of the lists are all gonna go down","width":192},{"text":"Not guranteed but on average the length of the lists are gonna go down so you sort of want that behavior to shrink your lists lengths","width":512},{"text":"There are tricks you could probably do to try and avoid redoing all of the hashing work but it is not worth it because  it is still going to be amertized and constant anyway","width":704},{"text":"There is an implicit time space tradeoff here","width":1280},{"text":"Typically you set your load factor to be something smaller then 1","width":320},{"text":"Which means that you have a lot of empty cells","width":256},{"text":"So I have a lot of just blank lists just sitting in this table","width":128},{"text":"Im using potentially more space, but the tradeoff is that I have faster running times","width":448},{"text":"However it gets a little bit fuzzy if you start thinking--think a little bit about what is the overhead?","width":448},{"text":"For every key value pair whats the overhead that you store to implement the data structure compared to just storing all the key value pairs directly.","width":512},{"text":"You have to store every key value pair, that's the space that you have to have, and then how much more space are you using?","width":384},{"text":"For a tree you've got pointers and the height and everything else","width":256},{"text":"For this you've got the linked list nodes and stuff","width":192},{"text":"For this next one you will have something different.","width":64},{"text":"So it's interesting to think about what your overhead is","width":192},{"text":"I wont do it here","width":256},{"text":"I really one cover this next collision resolution strategy so that you know what it is for the lab","width":512},{"text":"This falls in the category of open addressing or closed hashing","width":448},{"text":"And now we're saying no buckets","width":256},{"text":"You are not allowed to have buckets","width":128},{"text":"There can only ever be one thing in a slot in this array","width":320},{"text":"Now what do we do?","width":320},{"text":"We are gonna have an array I'm not gonna say what this part is for yet. This is where we are gonna put stuff","width":1792},{"text":"So we just have the array now","width":256},{"text":"We are gonna do the same inserts as before","width":192},{"text":"16 mod 7, 0 1 2 3 4 5 6","width":768},{"text":"so 16 mod 7 goes where? 2","width":640},{"text":"8 mod 7 goes...1","width":384},{"text":"4 mod 7...goes in 4","width":384},{"text":"13 mod 7 goes in 6","width":320},{"text":"29 mod 7... oh wait","width":512},{"text":"How do we put it there?","width":128},{"text":"There's something there","width":128},{"text":"Boot it?","width":128},{"text":"Over write it?","width":64},{"text":"No we've lost data","width":192},{"text":"So what are we gonna do","width":576},{"text":"We are gonna notice that we've got space","width":448},{"text":"In our array","width":128},{"text":"Our array isn't full, there is still somewhere we could put this","width":256},{"text":"The key is gonna be placing it somewhere we know how to get back to it later","width":384},{"text":"and the simplest strategy is called probing","width":384},{"text":"Because what you are gonna do is you are gonna look in the table for some other spot that you could place this element, so you are gonna probe forward in the table","width":448},{"text":"And the simplest strategy is called linear probing which is just look at the next slot","width":320},{"text":"and try and place it there","width":128},{"text":"So when we are inserting 29","width":256},{"text":"That hashed here, which is currently occupied","width":320}]
[{"text":"so um what is this probability ","width":178},{"text":"well this is a worked example of the probability of a second failure","width":508},{"text":"is determined by the time to repair ","width":404},{"text":"divided by the mean time to failure divided by the number of disks","width":222},{"text":"and so if we plug in some typical values","width":227},{"text":"then we discover we've got a .009 chance of losing all of our data","width":546},{"text":"of that one of our disks dying whilst we're trying to recover our data","width":423},{"text":"it turns out that this mathematic figure is completely wrong","width":530},{"text":"it's not true in real life","width":395},{"text":"have you heard of murphy's law?","width":293},{"text":"murphy's law turns out to be extremely true for RAID disks ","width":359},{"text":"if something goes wrong, and there's other versions that say look if bad things happened once they're going to happen a second time","width":499},{"text":"or similar chinese proverbs as well","width":249},{"text":"why do you think the second disk dying whilst you're trying to recover is much more likely? there's actually two reasons, yes!","width":738},{"text":"yeah, thanks! look if disks had been bought from the same supplier, from the same batch, they're probably going to expire about the same time","width":496},{"text":"you know that little bit of grit inside the bearing that caused it to heat up and eventually warp?","width":296},{"text":"well that's caused because part of the manufacturing process is sprinkling grit on that spot and if one disk fails after 2000 hours, then probably another one as well","width":757},{"text":"the second thing to realize is that this repair process is really hard on the disks ","width":512},{"text":"you're going to reading out all of the data and probably trying to use the disk array for whatever production requirement you have at the same time","width":708},{"text":"so not only are you trying to sweep through the entire data set, your production process is saying","width":322},{"text":"yeah i need this data i need this data i need to write this I need to write this","width":183},{"text":"so you're stressing your disk drive","width":207},{"text":"much more than normal operation","width":759},{"text":"so another department inside this campus experienced this very same problem and it turned out that even though they had RAID, their two mistakes were a) to assume that RAID was perfect and b) not to have any monitoring to tell them when their RAID had failed","width":1390},{"text":"and so they experienced this very problem that during the repair, a second disk failed, and that was it","width":658},{"text":"it was gone","width":140},{"text":"and their backup was several months old, whoops","width":483},{"text":"alright, so um back to our RAID 3","width":342},{"text":"our performance is only so so because we need to update the check disk for every write","width":510},{"text":"right so lastly let's look at raid 5","width":404},{"text":"and raid 5 mixes it up a little bit ","width":208},{"text":"or actually literally","width":136},{"text":"here's the trick: for different data blocks we'll put the parity bit inside a different disk ","width":568},{"text":"we're interleaving the parity across multiple disks ","width":374},{"text":"so now we've reduced the bottleneck of storing all the parity information on the same disk and in fact we can now allow multiple reads and writes in parallel","width":916},{"text":"right so that's the short intro to raid 1 3 and 5","width":269},{"text":"that's all i expect you to know for cs 241","width":235},{"text":"let's stand back even further and actually talk about a bigger problem which is ","width":404},{"text":"okay how do i make something which is even bigger than a data center","width":287},{"text":"if i want to have a file system that spans planetary sized data","width":356},{"text":"so for example, you're google and you're storing everybody's gmail and you thought running out of storage space on EWS was bad","width":783},{"text":"this is from a google engineer","width":135},{"text":"you know you have a large storage system when you get notified at a petabyte of storage left","width":439},{"text":"I wish I had that problem on my laptop, right? I'm sorry, I've only got a petabyte of storage left. so how could we build a file system that is at this scale? ","width":1051},{"text":"okay, well what kind of issues do we have? yeah we know that disks fail ","width":223},{"text":"and in fact google takes the approach of buying very low commodity hardware and expecting failure and instead putting software and algorithmic refinements on top of commodity hardware ","width":1105},{"text":"the stories of when google was scaling up other companies couldn't believe the trash that google was bringing into data centers","width":481},{"text":"why are you bringing in that old dell box? don't you expect it to fail? the google guys were like \"yes! we expect it to fail! but that's okay\"","width":384}]
[{"text":"Particularly, if i set this to some value that is less than one, the expected length of an arbitrary list is smaller than one. I'm expecting the length of the lists that i pick most to be 0 or 1. That's obviously pretty good because if there's only one thing in the list it's just dereference head and you're done.","width":1791.6666666269302},{"text":"So this resizing is super important to be able to guarantee that these lists are really small. What do we actually do when we do a resize? Similar to the argument that we had before, where we wanted to do a geometric resize of the table, we're going to do a geometric resize of the table as well with one little modification just to make sure things work out nicely. It turns out if you have hash table sizes that are prime numbers you get a little bit better hash function performance when you do the modulo when you have mods that are sort of relatively prime. We're going to create a new array.","width":3135.6666666269302},{"text":"Of two times m up to the nearest prime. The function that does that is already given to you for the lab. I want to double the array size and round up to the next closest prime because I want my table sizes to be prime. There's some theory that I'm not going to go into. The tables are a little bit better behaved. Step one if to make the new storage and now I've got to move everything over. This is your question. If I'm resizing when m is going to increase the value for my hash function are now being modded by a different number which means potentially every key has a different location. So when I'm resizing I cannot just do a copy over all the list. I cannot just move over the list into the same positions because those elements may not go there anymore. When I do a resize, I need to go through all the elements and rehash them and insert them into my new table. I sort of redo the process of inserting them. ","width":6975.66666662693},{"text":"This is were that doubling comes into play where we really need that doubling. When I do the resize, I make a new thing twice m up to the new prime and then I do a bunch of insertions so I want to make sure I'm not doing that very often. It's sort of the same kind of intuition when we were doing the same kind of resizing on regular arrays.","width":2175.6666666269302},{"text":"When I need to resize thie table I got through I make the array and go through all of the elements in my old table and insert them into my new storage.","width":1023.6666666269302},{"text":"Once I'm done i can throw away the old table. THe key step is to rehash everything.","width":831.6666666269302},{"text":"From the old table to the new table.","width":1151.6666666269302},{"text":"Once I've got that array i just swap it in. I make the array with the right number of things, I got through all the things i used to have and insert into bigger array. Throw away old array take ownership of new array and everything has be switched over.","width":1768.6666666269302},{"text":"I have to rehash everything so i can locate them again. THe mod changed. There index is different i have to make sure I'm rehashing.","width":895.6666666269302},{"text":"So then if we're doing that, then what are our running times. We looking at the worst case. If somebody gave you a bad hash function that doesn't satisfy the uniform criteria. WHat's the worst case running time of insert? ","width":2239.6666666269302},{"text":"Hash function is indeed constant time and deterministic. It just is not uniform. What's the worst case time of insertion? What do I do? I hash it to some value. i mod the size and insert into list. How long does it take? It didn't really matter. Still constant time.","width":2316.6666666269302},{"text":"But now what if i go to find something. THis could be linear now with a bad hash function. The hash function could choose to put everything in the first cell. Non uniform hash function just a linked list now. Remove could be linear as well.","width":1663.6666666269302},{"text":"That assumption is really important.","width":447.66666662693024},{"text":"If our hash function is really bad everything ends up in the same cell and we can't make any of the arguments we just made. SO uniform assumption is super important. Let's take about the average running time, not worst case running time. I'm talking about average case. In expectation what is the running time. Insert doesn't really change. Still constant time. Doesn't actually have any average. What about find now? Find is O(1)* now.","width":2943.6666666269302},{"text":"On average and the reason is i made an argument about the average length of a list is the load factor. SO the average case here is constant time.","width":1023.6666666269302}]